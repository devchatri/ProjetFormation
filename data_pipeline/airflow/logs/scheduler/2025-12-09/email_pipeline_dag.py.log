[2025-12-09T08:18:43.323+0000] {processor.py:161} INFO - Started process (PID=194) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:18:43.333+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:18:43.336+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:18:43.336+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:18:43.458+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:18:43.649+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:18:43.649+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:18:43.799+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:18:43.799+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:18:43.932+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.676 seconds
[2025-12-09T08:19:14.237+0000] {processor.py:161} INFO - Started process (PID=327) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:19:14.244+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:19:14.249+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:19:14.247+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:19:14.376+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:19:14.973+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:19:14.972+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:19:15.045+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:19:15.044+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:19:15.109+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.914 seconds
[2025-12-09T08:19:46.523+0000] {processor.py:161} INFO - Started process (PID=546) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:19:46.538+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:19:46.568+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:19:46.559+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:19:47.118+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:19:47.582+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:19:47.581+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:19:47.799+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:19:47.798+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:19:47.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.675 seconds
[2025-12-09T08:20:18.558+0000] {processor.py:161} INFO - Started process (PID=614) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:20:18.574+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:20:18.594+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:20:18.583+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:20:18.737+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:20:17.862+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:20:17.861+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:20:18.481+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:20:18.480+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:20:18.694+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.375 seconds
[2025-12-09T08:20:49.424+0000] {processor.py:161} INFO - Started process (PID=630) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:20:49.443+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:20:49.481+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:20:49.463+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:20:50.536+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:21:00.619+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:21:00.590+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:21:01.292+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:21:01.278+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:21:01.873+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 12.611 seconds
[2025-12-09T08:21:32.824+0000] {processor.py:161} INFO - Started process (PID=656) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:21:32.873+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:21:32.914+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:21:32.893+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:21:34.024+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:21:39.949+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:21:39.927+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:21:40.761+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:21:40.737+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:21:42.175+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 9.841 seconds
[2025-12-09T08:22:12.722+0000] {processor.py:161} INFO - Started process (PID=661) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:22:12.727+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:22:12.733+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:22:12.728+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:22:12.868+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:22:17.947+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:22:17.914+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:22:19.208+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:22:19.149+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:22:21.471+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 11.139 seconds
[2025-12-09T08:22:52.075+0000] {processor.py:161} INFO - Started process (PID=666) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:22:52.087+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:22:52.097+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:22:52.090+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:22:52.337+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:23:12.584+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:23:12.567+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:23:13.075+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:23:13.065+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:23:14.079+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 22.028 seconds
[2025-12-09T08:23:45.182+0000] {processor.py:161} INFO - Started process (PID=671) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:23:45.215+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:23:45.268+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:23:45.245+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:23:46.327+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:23:48.427+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:23:48.402+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:23:46.895+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:23:46.880+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:23:47.436+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 4.700 seconds
[2025-12-09T08:24:18.734+0000] {processor.py:161} INFO - Started process (PID=676) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:24:18.770+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:24:16.498+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:24:18.795+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:24:17.244+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:24:23.271+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:24:23.254+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:24:24.198+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:24:24.173+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:24:25.568+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 9.392 seconds
[2025-12-09T08:24:56.448+0000] {processor.py:161} INFO - Started process (PID=687) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:24:56.647+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:24:57.063+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:24:56.789+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:24:58.551+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:25:27.628+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T08:26:01.490+0000] {processor.py:161} INFO - Started process (PID=718) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:27:04.551+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:27:37.015+0000] {processor.py:161} INFO - Started process (PID=722) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:27:37.062+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:27:37.105+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:27:37.084+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:27:38.859+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:27:41.950+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:27:41.950+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:27:42.185+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:27:42.181+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:27:42.369+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 5.688 seconds
[2025-12-09T08:28:13.131+0000] {processor.py:161} INFO - Started process (PID=983) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:28:13.157+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:28:13.173+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:28:13.163+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:28:13.783+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:28:15.375+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:28:15.358+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:28:15.632+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:28:15.625+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:28:15.837+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.756 seconds
[2025-12-09T08:28:48.450+0000] {processor.py:161} INFO - Started process (PID=1006) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:28:51.085+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:28:54.652+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:28:52.691+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:31:05.679+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:29:48.278+0000] {timeout.py:68} ERROR - Process timed out, PID: 1006
[2025-12-09T08:55:30.150+0000] {processor.py:161} INFO - Started process (PID=170) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:55:30.153+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:55:30.156+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:55:30.155+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:55:30.207+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:55:30.271+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:55:30.271+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:55:30.311+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:55:30.311+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:55:30.347+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.204 seconds
[2025-12-09T08:56:00.456+0000] {processor.py:161} INFO - Started process (PID=242) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:56:00.460+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:56:00.466+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:56:00.464+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:56:00.603+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:56:01.144+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:56:01.143+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:56:01.225+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:56:01.225+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:56:01.274+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.832 seconds
[2025-12-09T08:56:31.380+0000] {processor.py:161} INFO - Started process (PID=599) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:56:31.383+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:56:31.386+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:56:31.385+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:56:31.505+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:56:31.786+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:56:31.786+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:56:31.857+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:56:31.856+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:56:31.915+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.547 seconds
[2025-12-09T08:57:02.265+0000] {processor.py:161} INFO - Started process (PID=1165) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:57:02.275+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:57:02.288+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:57:02.284+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:57:02.625+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:57:02.864+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:57:02.864+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:57:03.012+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:57:03.011+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:57:03.085+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.916 seconds
[2025-12-09T08:57:33.458+0000] {processor.py:161} INFO - Started process (PID=1499) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:57:33.463+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:57:33.473+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:57:33.469+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:57:33.834+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:57:34.155+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:57:34.155+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:57:34.335+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:57:34.334+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:57:34.451+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.062 seconds
[2025-12-09T08:58:04.555+0000] {processor.py:161} INFO - Started process (PID=1504) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:58:04.556+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:58:04.558+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:58:04.557+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:58:04.588+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:58:04.633+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:58:04.633+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:58:04.678+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:58:04.678+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:58:04.736+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.186 seconds
[2025-12-09T08:58:34.875+0000] {processor.py:161} INFO - Started process (PID=1509) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:58:34.879+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:58:34.882+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:58:34.881+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:58:34.946+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:58:35.006+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:58:35.005+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:58:35.052+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:58:35.051+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:58:35.090+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.227 seconds
[2025-12-09T08:59:05.280+0000] {processor.py:161} INFO - Started process (PID=1514) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:59:05.281+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:59:05.282+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:59:05.282+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:59:05.318+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:59:05.355+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:59:05.355+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:59:05.376+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:59:05.376+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:59:05.398+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.123 seconds
[2025-12-09T08:59:36.196+0000] {processor.py:161} INFO - Started process (PID=1519) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:59:36.197+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T08:59:36.198+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:59:36.198+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:59:36.221+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T08:59:36.251+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:59:36.251+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T08:59:36.272+0000] {logging_mixin.py:188} INFO - [2025-12-09T08:59:36.272+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T08:59:36.293+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.100 seconds
[2025-12-09T09:00:06.496+0000] {processor.py:161} INFO - Started process (PID=1582) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:00:06.499+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:00:06.501+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:00:06.501+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:00:06.548+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:00:06.593+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:00:06.593+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:00:06.622+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:00:06.621+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:00:06.653+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.165 seconds
[2025-12-09T09:00:37.573+0000] {processor.py:161} INFO - Started process (PID=1899) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:00:37.576+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:00:37.578+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:00:37.577+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:00:37.625+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:00:37.767+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:00:37.767+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:00:37.836+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:00:37.836+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:00:37.882+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.321 seconds
[2025-12-09T09:01:08.707+0000] {processor.py:161} INFO - Started process (PID=2546) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:01:08.711+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:01:08.713+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:01:08.713+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:01:08.776+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:01:08.853+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:01:08.852+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:01:08.886+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:01:08.885+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:01:08.938+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.240 seconds
[2025-12-09T09:01:39.936+0000] {processor.py:161} INFO - Started process (PID=2742) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:01:39.938+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:01:39.940+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:01:39.939+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:01:39.973+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:01:40.010+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:01:40.010+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:01:40.036+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:01:40.035+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:01:40.056+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.125 seconds
[2025-12-09T09:02:10.946+0000] {processor.py:161} INFO - Started process (PID=2747) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:02:10.947+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:02:10.948+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:02:10.948+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:02:10.980+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:02:11.019+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:02:11.018+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:02:11.048+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:02:11.048+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:02:11.071+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.130 seconds
[2025-12-09T09:02:42.109+0000] {processor.py:161} INFO - Started process (PID=2752) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:02:42.110+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:02:42.111+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:02:42.111+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:02:42.142+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:02:42.179+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:02:42.179+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:02:42.205+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:02:42.205+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:02:42.227+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.122 seconds
[2025-12-09T09:03:13.115+0000] {processor.py:161} INFO - Started process (PID=2757) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:03:13.117+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:03:13.119+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:03:13.118+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:03:13.149+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:03:13.186+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:03:13.186+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:03:13.211+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:03:13.210+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:03:13.236+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.126 seconds
[2025-12-09T09:03:44.234+0000] {processor.py:161} INFO - Started process (PID=2762) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:03:44.236+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:03:44.237+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:03:44.237+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:03:44.262+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:03:44.296+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:03:44.296+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:03:44.322+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:03:44.322+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:03:44.345+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.115 seconds
[2025-12-09T09:04:14.970+0000] {processor.py:161} INFO - Started process (PID=2767) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:04:14.973+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:04:14.974+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:04:14.974+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:04:15.002+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:04:15.039+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:04:15.039+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:04:15.063+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:04:15.063+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:04:15.093+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.127 seconds
[2025-12-09T09:04:46.038+0000] {processor.py:161} INFO - Started process (PID=2772) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:04:46.040+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:04:46.041+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:04:46.041+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:04:46.066+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:04:46.097+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:04:46.097+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:04:46.120+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:04:46.120+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:04:46.143+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.108 seconds
[2025-12-09T09:05:17.166+0000] {processor.py:161} INFO - Started process (PID=2777) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:05:17.175+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:05:17.178+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:05:17.177+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:05:17.218+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:05:17.267+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:05:17.266+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:05:17.306+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:05:17.305+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:05:17.350+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.192 seconds
[2025-12-09T09:05:48.216+0000] {processor.py:161} INFO - Started process (PID=2782) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:05:48.217+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:05:48.218+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:05:48.218+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:05:48.250+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:05:48.280+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:05:48.280+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:05:48.301+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:05:48.301+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:05:48.328+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.116 seconds
[2025-12-09T09:06:19.272+0000] {processor.py:161} INFO - Started process (PID=2787) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:06:19.274+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:06:19.276+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:06:19.275+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:06:19.306+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:06:19.337+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:06:19.337+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:06:19.368+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:06:19.367+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:06:19.390+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.120 seconds
[2025-12-09T09:06:50.316+0000] {processor.py:161} INFO - Started process (PID=2792) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:06:50.318+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:06:50.320+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:06:50.319+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:06:50.343+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:06:50.378+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:06:50.377+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:06:50.403+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:06:50.403+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:06:50.424+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.112 seconds
[2025-12-09T09:07:21.387+0000] {processor.py:161} INFO - Started process (PID=2797) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:07:21.388+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:07:21.389+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:07:21.389+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:07:21.421+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:07:21.451+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:07:21.451+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:07:21.475+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:07:21.475+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:07:21.496+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.113 seconds
[2025-12-09T09:07:52.425+0000] {processor.py:161} INFO - Started process (PID=2802) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:07:52.426+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:07:52.427+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:07:52.427+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:07:52.451+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:07:52.482+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:07:52.482+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:07:52.504+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:07:52.504+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:07:52.526+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.105 seconds
[2025-12-09T09:08:23.192+0000] {processor.py:161} INFO - Started process (PID=2807) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:08:23.193+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:08:23.194+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:08:23.194+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:08:23.220+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:08:23.250+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:08:23.250+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:08:23.271+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:08:23.270+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:08:23.291+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.102 seconds
[2025-12-09T09:08:53.372+0000] {processor.py:161} INFO - Started process (PID=2812) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:08:53.374+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:08:53.376+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:08:53.375+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:08:53.419+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:08:53.486+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:08:53.486+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:08:53.533+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:08:53.532+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:08:53.577+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.210 seconds
[2025-12-09T09:09:24.425+0000] {processor.py:161} INFO - Started process (PID=2817) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:09:24.428+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:09:24.430+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:09:24.429+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:09:24.492+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:09:24.532+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:09:24.532+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:09:24.565+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:09:24.565+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:09:24.593+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.176 seconds
[2025-12-09T09:09:55.641+0000] {processor.py:161} INFO - Started process (PID=2822) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:09:55.643+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:09:55.644+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:09:55.643+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:09:55.668+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:09:55.707+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:09:55.707+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:09:55.736+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:09:55.735+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:09:55.768+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.132 seconds
[2025-12-09T09:10:26.681+0000] {processor.py:161} INFO - Started process (PID=2827) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:10:26.687+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:10:26.691+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:10:26.690+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:10:26.790+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:10:26.854+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:10:26.854+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:10:26.894+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:10:26.894+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:10:26.927+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.260 seconds
[2025-12-09T09:10:57.811+0000] {processor.py:161} INFO - Started process (PID=2832) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:10:57.813+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:10:57.814+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:10:57.814+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:10:57.840+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:10:57.874+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:10:57.874+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:10:57.900+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:10:57.900+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:10:57.938+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.131 seconds
[2025-12-09T09:11:28.970+0000] {processor.py:161} INFO - Started process (PID=2842) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:11:28.976+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:11:28.979+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:11:28.978+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:11:29.033+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:11:29.108+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:11:29.108+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:11:29.151+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:11:29.151+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:11:29.188+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.233 seconds
[2025-12-09T09:12:01.412+0000] {processor.py:161} INFO - Started process (PID=2973) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:12:01.494+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:12:01.552+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:12:01.532+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:12:05.311+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:12:34.909+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T09:13:19.762+0000] {processor.py:161} INFO - Started process (PID=3012) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:13:20.082+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:13:21.014+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:13:20.276+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:13:30.065+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:14:01.932+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:14:01.900+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T09:14:03.116+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:14:03.084+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T09:14:04.144+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 47.112 seconds
[2025-12-09T09:14:35.002+0000] {processor.py:161} INFO - Started process (PID=3069) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:14:32.936+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:14:33.302+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:14:33.141+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:14:40.047+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:15:16.538+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T09:15:48.435+0000] {processor.py:161} INFO - Started process (PID=3074) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:15:48.511+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:15:48.682+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:15:48.631+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:16:02.236+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:26:32.069+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T09:32:27.323+0000] {processor.py:161} INFO - Started process (PID=3117) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:32:27.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T09:32:27.389+0000] {logging_mixin.py:188} INFO - [2025-12-09T09:32:27.375+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:32:32.135+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T09:33:02.178+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T10:22:58.333+0000] {processor.py:161} INFO - Started process (PID=3122) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:22:59.313+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:22:59.719+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:22:59.540+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:23:32.953+0000] {processor.py:161} INFO - Started process (PID=3153) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:23:32.959+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:23:32.970+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:23:32.965+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:23:33.412+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:23:35.046+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:23:35.045+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:23:35.244+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:23:35.242+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:23:35.393+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.479 seconds
[2025-12-09T10:24:06.401+0000] {processor.py:161} INFO - Started process (PID=3159) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:24:06.542+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:24:06.908+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:24:06.780+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:24:10.185+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:24:29.835+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:24:29.807+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:24:29.983+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:24:29.969+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:24:30.622+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 26.800 seconds
[2025-12-09T10:25:01.917+0000] {processor.py:161} INFO - Started process (PID=3164) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:25:01.927+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:25:01.941+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:25:01.929+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:25:02.691+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:25:31.277+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T10:26:01.788+0000] {processor.py:161} INFO - Started process (PID=3169) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:26:01.853+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:26:01.890+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:26:01.872+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:26:02.572+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:26:04.241+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:26:04.195+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:26:05.634+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:26:05.572+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:26:08.987+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 7.220 seconds
[2025-12-09T10:26:40.020+0000] {processor.py:161} INFO - Started process (PID=3174) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:26:40.055+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:26:40.088+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:26:40.056+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:26:41.436+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:27:03.435+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:27:03.406+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:27:03.651+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:27:03.639+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:27:04.896+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 27.475 seconds
[2025-12-09T10:27:35.773+0000] {processor.py:161} INFO - Started process (PID=3179) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:27:35.784+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:27:35.794+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:27:35.787+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:27:35.932+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:27:42.271+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:27:42.259+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:27:42.359+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:27:42.358+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:27:42.452+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 6.703 seconds
[2025-12-09T10:28:13.638+0000] {processor.py:161} INFO - Started process (PID=3184) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:28:13.664+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:28:13.700+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:28:13.682+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:28:14.199+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:28:43.010+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T10:29:14.017+0000] {processor.py:161} INFO - Started process (PID=3202) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:29:14.028+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:29:14.037+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:29:14.030+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:29:15.834+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:29:44.448+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T10:30:16.218+0000] {processor.py:161} INFO - Started process (PID=3216) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:30:16.738+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:30:17.046+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:30:16.834+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:30:27.576+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:30:32.316+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:30:32.305+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:30:32.424+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:30:32.409+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:30:34.392+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 18.402 seconds
[2025-12-09T10:31:07.753+0000] {processor.py:161} INFO - Started process (PID=3228) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:31:05.313+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:31:05.696+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:31:05.501+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:31:09.151+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:31:37.605+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:31:37.594+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:31:38.223+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:31:38.208+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:31:39.564+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 38.525 seconds
[2025-12-09T10:32:10.433+0000] {processor.py:161} INFO - Started process (PID=3233) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:32:10.441+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:32:10.448+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:32:10.443+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:32:10.754+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:32:15.308+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:32:15.264+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:32:15.760+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:32:15.743+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:32:18.393+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 10.743 seconds
[2025-12-09T10:32:49.269+0000] {processor.py:161} INFO - Started process (PID=3238) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:32:49.289+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:32:49.315+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:32:49.301+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:32:49.466+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:33:17.545+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T10:33:48.869+0000] {processor.py:161} INFO - Started process (PID=3243) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:33:48.915+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:33:49.094+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:33:48.962+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:33:51.881+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:34:20.298+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T10:34:50.776+0000] {processor.py:161} INFO - Started process (PID=3248) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:34:50.804+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:34:50.836+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:34:50.813+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:34:52.323+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:35:21.480+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T10:35:50.474+0000] {processor.py:161} INFO - Started process (PID=3260) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:35:50.478+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:35:50.484+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:35:50.480+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:35:50.621+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:36:18.941+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T10:36:52.397+0000] {processor.py:161} INFO - Started process (PID=3297) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:36:52.455+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:36:52.728+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:36:52.555+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:36:59.906+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:37:29.601+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:37:29.586+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:37:30.120+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:37:30.102+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:37:28.750+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 41.955 seconds
[2025-12-09T10:37:59.466+0000] {processor.py:161} INFO - Started process (PID=3302) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:37:59.508+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:37:57.044+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:37:59.635+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:38:01.433+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:38:31.575+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T10:39:03.812+0000] {processor.py:161} INFO - Started process (PID=3307) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:39:03.831+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:39:03.847+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:39:03.834+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:39:04.407+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:39:32.238+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:39:32.222+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:39:32.513+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:39:32.504+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:39:32.843+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 31.795 seconds
[2025-12-09T10:40:04.069+0000] {processor.py:161} INFO - Started process (PID=3312) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:40:04.100+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:40:04.132+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:40:04.107+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:40:05.837+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:40:31.298+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:40:31.276+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:40:31.436+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:40:31.429+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:40:31.624+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 30.365 seconds
[2025-12-09T10:41:08.169+0000] {processor.py:161} INFO - Started process (PID=3359) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:41:08.309+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:41:08.519+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:41:08.459+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:41:14.871+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:41:39.956+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:41:39.937+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:41:40.383+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:41:40.367+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:41:40.801+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 40.184 seconds
[2025-12-09T10:42:11.439+0000] {processor.py:161} INFO - Started process (PID=3364) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:42:11.569+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:42:12.058+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:42:11.750+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:42:18.077+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:43:37.287+0000] {processor.py:161} INFO - Started process (PID=3367) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:43:37.345+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:43:37.485+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:43:37.411+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:43:38.250+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:44:08.601+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:44:08.580+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:44:08.769+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:44:08.753+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:44:09.540+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 36.505 seconds
[2025-12-09T10:44:44.412+0000] {processor.py:161} INFO - Started process (PID=3387) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:44:48.857+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:44:52.542+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:44:52.482+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:44:51.971+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:45:08.981+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:45:08.927+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:45:19.150+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:45:19.109+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:45:21.422+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 41.145 seconds
[2025-12-09T10:45:51.856+0000] {processor.py:161} INFO - Started process (PID=3392) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:45:51.893+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:45:51.911+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:45:51.899+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:46:17.253+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:47:12.419+0000] {processor.py:161} INFO - Started process (PID=3395) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:47:12.441+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:47:12.463+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:47:12.451+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:47:13.096+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:50:14.263+0000] {processor.py:161} INFO - Started process (PID=3400) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:50:14.274+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:50:14.283+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:50:14.276+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:50:14.458+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:53:48.722+0000] {processor.py:161} INFO - Started process (PID=3407) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:53:48.727+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:53:48.740+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:53:48.735+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:53:49.238+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:53:47.886+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:53:47.886+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:53:47.961+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:53:47.960+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:53:48.021+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.081 seconds
[2025-12-09T10:54:18.378+0000] {processor.py:161} INFO - Started process (PID=3412) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:54:18.383+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:54:18.386+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:54:18.385+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:54:18.446+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:54:18.511+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:54:18.511+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:54:18.550+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:54:18.550+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:54:18.594+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.243 seconds
[2025-12-09T10:54:49.249+0000] {processor.py:161} INFO - Started process (PID=3417) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:54:49.251+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:54:49.252+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:54:49.252+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:54:49.318+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:54:49.439+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:54:49.439+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:54:49.477+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:54:49.477+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:54:49.515+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.272 seconds
[2025-12-09T10:55:19.914+0000] {processor.py:161} INFO - Started process (PID=3422) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:55:19.916+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:55:19.917+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:55:19.917+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:55:19.952+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:55:20.009+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:55:20.008+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:55:20.057+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:55:20.057+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:55:20.082+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.173 seconds
[2025-12-09T10:55:50.252+0000] {processor.py:161} INFO - Started process (PID=3427) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:55:50.254+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:55:50.256+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:55:50.255+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:55:50.320+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:55:50.465+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:55:50.464+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:55:50.508+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:55:50.507+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:55:50.548+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.302 seconds
[2025-12-09T10:56:20.744+0000] {processor.py:161} INFO - Started process (PID=3432) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:56:20.746+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:56:20.747+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:56:20.747+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:56:20.777+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:56:20.815+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:56:20.815+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:56:20.838+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:56:20.838+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:56:20.899+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.158 seconds
[2025-12-09T10:56:50.976+0000] {processor.py:161} INFO - Started process (PID=3437) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:56:50.978+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:56:50.980+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:56:50.979+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:56:51.014+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:56:51.083+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:56:51.083+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:56:51.220+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:56:51.220+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:56:51.287+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.317 seconds
[2025-12-09T10:57:21.534+0000] {processor.py:161} INFO - Started process (PID=3442) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:57:21.537+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:57:21.538+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:57:21.538+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:57:21.581+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:57:21.705+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:57:21.705+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:57:21.728+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:57:21.728+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:57:21.750+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.241 seconds
[2025-12-09T10:57:51.839+0000] {processor.py:161} INFO - Started process (PID=3447) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:57:51.841+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:57:51.842+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:57:51.842+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:57:51.869+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:57:51.902+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:57:51.901+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:57:51.923+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:57:51.923+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:57:51.948+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.114 seconds
[2025-12-09T10:58:22.078+0000] {processor.py:161} INFO - Started process (PID=3452) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:58:22.080+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:58:22.081+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:58:22.081+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:58:22.110+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:58:22.157+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:58:22.157+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:58:22.184+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:58:22.183+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:58:22.213+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.138 seconds
[2025-12-09T10:58:53.155+0000] {processor.py:161} INFO - Started process (PID=3457) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:58:53.157+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:58:53.158+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:58:53.158+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:58:53.186+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:58:53.221+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:58:53.220+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:58:53.257+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:58:53.256+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:58:53.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.156 seconds
[2025-12-09T10:59:23.382+0000] {processor.py:161} INFO - Started process (PID=3462) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:59:23.387+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:59:23.388+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:59:23.388+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:59:23.417+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:59:23.448+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:59:23.447+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:59:23.478+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:59:23.477+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:59:23.557+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.180 seconds
[2025-12-09T10:59:54.327+0000] {processor.py:161} INFO - Started process (PID=3467) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:59:54.328+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T10:59:54.330+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:59:54.330+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:59:54.364+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T10:59:54.411+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:59:54.411+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T10:59:54.518+0000] {logging_mixin.py:188} INFO - [2025-12-09T10:59:54.517+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T10:59:54.621+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.319 seconds
[2025-12-09T11:00:24.758+0000] {processor.py:161} INFO - Started process (PID=3472) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:00:24.777+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:00:24.790+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:00:24.784+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:00:24.855+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:00:24.891+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:00:24.891+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:00:24.934+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:00:24.933+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:00:24.985+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.239 seconds
[2025-12-09T11:00:55.139+0000] {processor.py:161} INFO - Started process (PID=3477) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:00:55.141+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:00:55.142+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:00:55.142+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:00:55.172+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:00:55.331+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:00:55.330+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:00:55.531+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:00:55.531+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:00:55.604+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.469 seconds
[2025-12-09T11:01:25.820+0000] {processor.py:161} INFO - Started process (PID=3482) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:01:25.874+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:01:25.891+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:01:25.886+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:01:25.952+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:01:25.981+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:01:25.980+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:01:26.038+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:01:26.037+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:01:26.060+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.270 seconds
[2025-12-09T11:01:56.328+0000] {processor.py:161} INFO - Started process (PID=3487) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:01:56.330+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:01:56.332+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:01:56.331+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:01:56.363+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:01:56.398+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:01:56.397+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:01:56.422+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:01:56.422+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:01:56.447+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.126 seconds
[2025-12-09T11:02:26.716+0000] {processor.py:161} INFO - Started process (PID=3492) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:02:26.718+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:02:26.719+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:02:26.719+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:02:26.742+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:02:26.770+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:02:26.770+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:02:26.794+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:02:26.794+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:02:26.832+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.118 seconds
[2025-12-09T11:02:56.896+0000] {processor.py:161} INFO - Started process (PID=3497) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:02:56.899+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:02:56.900+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:02:56.899+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:02:56.925+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:02:56.972+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:02:56.972+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:02:57.015+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:02:57.014+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:02:57.103+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.210 seconds
[2025-12-09T11:03:27.222+0000] {processor.py:161} INFO - Started process (PID=3502) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:03:27.234+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:03:27.245+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:03:27.240+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:03:27.321+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:03:27.354+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:03:27.354+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:03:27.377+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:03:27.376+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:03:27.418+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.200 seconds
[2025-12-09T11:05:58.074+0000] {processor.py:161} INFO - Started process (PID=171) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:05:58.077+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:05:58.084+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:05:58.080+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:05:58.146+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:05:58.200+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:05:58.199+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:05:58.240+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:05:58.239+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:05:58.280+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.211 seconds
[2025-12-09T11:06:28.485+0000] {processor.py:161} INFO - Started process (PID=176) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:06:28.503+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:06:28.525+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:06:28.515+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:06:28.688+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:06:29.265+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:06:29.265+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:06:29.344+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:06:29.343+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:06:29.400+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.955 seconds
[2025-12-09T11:06:59.895+0000] {processor.py:161} INFO - Started process (PID=189) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:06:59.898+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:06:59.900+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:06:59.899+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:06:59.928+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:06:59.961+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:06:59.961+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:06:59.986+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:06:59.986+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:07:00.016+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.125 seconds
[2025-12-09T11:07:30.406+0000] {processor.py:161} INFO - Started process (PID=194) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:07:30.418+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:07:30.424+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:07:30.423+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:07:30.544+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:07:30.633+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:07:30.633+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:07:30.724+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:07:30.723+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:07:30.760+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.361 seconds
[2025-12-09T11:08:01.237+0000] {processor.py:161} INFO - Started process (PID=199) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:08:01.240+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:08:01.242+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:08:01.242+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:08:01.310+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:08:01.441+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:08:01.441+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:08:01.528+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:08:01.528+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:08:01.564+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.333 seconds
[2025-12-09T11:08:32.229+0000] {processor.py:161} INFO - Started process (PID=204) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:08:32.239+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:08:32.249+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:08:32.246+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:08:32.304+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:08:32.395+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:08:32.395+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:08:32.498+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:08:32.497+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:08:32.538+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.316 seconds
[2025-12-09T11:09:02.797+0000] {processor.py:161} INFO - Started process (PID=348) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:09:02.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:09:02.825+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:09:02.821+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:09:03.013+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:09:03.158+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:09:03.158+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:09:03.221+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:09:03.221+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:09:03.263+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.511 seconds
[2025-12-09T11:09:34.158+0000] {processor.py:161} INFO - Started process (PID=566) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:09:34.176+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:09:34.190+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:09:34.185+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:09:34.550+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:09:35.053+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:09:35.053+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:09:35.262+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:09:35.261+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:09:35.377+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.365 seconds
[2025-12-09T11:13:10.127+0000] {processor.py:161} INFO - Started process (PID=635) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:13:10.174+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:13:10.261+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:13:10.204+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:13:42.071+0000] {processor.py:161} INFO - Started process (PID=909) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:13:42.079+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:13:42.124+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:13:42.099+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:13:42.550+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:13:43.035+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:13:43.034+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:13:43.406+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:13:43.402+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:13:43.688+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.734 seconds
[2025-12-09T11:14:15.699+0000] {processor.py:161} INFO - Started process (PID=1128) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:14:15.854+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:14:16.355+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:14:16.013+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:14:30.573+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:16:45.179+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T11:17:16.325+0000] {processor.py:161} INFO - Started process (PID=1138) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:17:16.408+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:17:16.519+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:17:16.475+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:17:18.856+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:17:20.717+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:17:20.715+0000] {taskinstance.py:2700} ERROR - {'DAG Id': 'email_intelligence_pipeline', 'Task Id': 'quality_assurance_gate', 'Run Id': 'manual__2025-12-09T11:06:30.879194+00:00', 'Hostname': '202880cbc844'}
[2025-12-09T11:17:21.236+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:17:21.235+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=email_intelligence_pipeline, task_id=quality_assurance_gate, execution_date=20251209T110630, start_date=20251209T111353, end_date=20251209T111720
[2025-12-09T11:17:21.589+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: email_intelligence_pipeline.quality_assurance_gate manual__2025-12-09T11:06:30.879194+00:00 [failed]> in state failed
[2025-12-09T11:17:22.228+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:17:22.227+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:17:22.519+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:17:22.514+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:17:22.705+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 7.356 seconds
[2025-12-09T11:17:54.552+0000] {processor.py:161} INFO - Started process (PID=1187) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:17:54.637+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:17:55.041+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:17:54.875+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:18:06.963+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:18:40.560+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T11:19:30.303+0000] {processor.py:161} INFO - Started process (PID=1199) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:19:30.333+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:19:30.384+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:19:30.336+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:19:36.491+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:20:12.056+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T11:21:01.678+0000] {processor.py:161} INFO - Started process (PID=1204) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:21:02.901+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:21:10.173+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:21:04.216+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:27:10.486+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:24:37.618+0000] {timeout.py:68} ERROR - Process timed out, PID: 1204
[2025-12-09T11:30:45.417+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:30:15.700+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/email_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 342, in parse
    new_module = importlib.util.module_from_spec(spec)
  File "<frozen importlib._bootstrap>", line 562, in module_from_spec
  File "<frozen importlib._bootstrap>", line 541, in _init_module_attrs
  File "<frozen importlib._bootstrap>", line 382, in cached
  File "<frozen importlib._bootstrap_external>", line 487, in _get_cached
  File "<frozen importlib._bootstrap_external>", line 380, in cache_from_source
  File "<frozen importlib._bootstrap_external>", line 129, in _path_split
  File "<frozen importlib._bootstrap_external>", line 129, in <genexpr>
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/email_pipeline_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 1204
[2025-12-09T11:30:46.056+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:30:49.556+0000] {processor.py:161} INFO - Started process (PID=1211) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:30:49.584+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:30:49.614+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:30:49.599+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:30:51.239+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:30:52.501+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:30:52.499+0000] {taskinstance.py:2700} ERROR - {'DAG Id': 'email_intelligence_pipeline', 'Task Id': 'daily_insights_aggregation', 'Run Id': 'manual__2025-12-09T11:08:31.321816+00:00', 'Hostname': '202880cbc844'}
[2025-12-09T11:30:52.297+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:30:52.296+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=email_intelligence_pipeline, task_id=daily_insights_aggregation, execution_date=20251209T110831, start_date=20251209T111333, end_date=20251209T113052
[2025-12-09T11:30:52.433+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: email_intelligence_pipeline.daily_insights_aggregation manual__2025-12-09T11:08:31.321816+00:00 [failed]> in state failed
[2025-12-09T11:30:53.709+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:30:53.709+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:30:53.861+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:30:53.860+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:30:53.966+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 5.370 seconds
[2025-12-09T11:31:24.564+0000] {processor.py:161} INFO - Started process (PID=1256) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:31:24.591+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:31:24.612+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:31:24.594+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:31:24.797+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:31:53.809+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T11:32:24.872+0000] {processor.py:161} INFO - Started process (PID=1268) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:32:24.904+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:32:24.922+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:32:24.908+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:32:25.318+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:32:37.298+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:32:37.283+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T11:32:38.110+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:32:38.080+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T11:32:38.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 14.087 seconds
[2025-12-09T11:33:09.140+0000] {processor.py:161} INFO - Started process (PID=1273) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:33:09.166+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:33:09.180+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:33:09.167+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:33:09.472+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:33:38.217+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T11:34:11.552+0000] {processor.py:161} INFO - Started process (PID=1278) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:34:11.662+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:34:11.785+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:34:11.700+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:34:13.855+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:35:04.392+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T11:35:44.120+0000] {processor.py:161} INFO - Started process (PID=1283) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:35:44.489+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T11:35:45.662+0000] {logging_mixin.py:188} INFO - [2025-12-09T11:35:45.173+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T11:35:49.656+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:29:26.536+0000] {processor.py:161} INFO - Started process (PID=171) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:29:26.537+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:29:26.539+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:29:26.538+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:29:26.566+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:29:26.687+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:29:26.687+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:29:26.703+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:29:26.703+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:29:26.722+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.189 seconds
[2025-12-09T12:29:56.780+0000] {processor.py:161} INFO - Started process (PID=176) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:29:56.781+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:29:56.782+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:29:56.782+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:29:56.802+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:29:56.905+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:29:56.905+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:29:56.948+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:29:56.947+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:29:56.987+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.210 seconds
[2025-12-09T12:30:27.689+0000] {processor.py:161} INFO - Started process (PID=181) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:30:27.691+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:30:27.693+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:30:27.692+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:30:27.713+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:30:27.746+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:30:27.745+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:30:27.766+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:30:27.765+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:30:27.784+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.100 seconds
[2025-12-09T12:30:58.645+0000] {processor.py:161} INFO - Started process (PID=186) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:30:58.646+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:30:58.647+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:30:58.647+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:30:58.671+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:30:58.706+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:30:58.705+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:30:58.734+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:30:58.734+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:30:58.758+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.117 seconds
[2025-12-09T12:31:28.903+0000] {processor.py:161} INFO - Started process (PID=191) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:31:28.904+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:31:28.905+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:31:28.905+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:31:28.927+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:31:28.963+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:31:28.963+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:31:29.109+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:31:29.109+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:31:29.130+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.231 seconds
[2025-12-09T12:31:59.635+0000] {processor.py:161} INFO - Started process (PID=196) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:31:59.637+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:31:59.638+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:31:59.638+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:31:59.661+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:31:59.689+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:31:59.689+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:31:59.708+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:31:59.707+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:31:59.725+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.092 seconds
[2025-12-09T12:32:30.555+0000] {processor.py:161} INFO - Started process (PID=201) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:32:30.556+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:32:30.557+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:32:30.557+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:32:30.573+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:32:30.602+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:32:30.601+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:32:30.621+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:32:30.621+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:32:30.644+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.092 seconds
[2025-12-09T12:33:01.622+0000] {processor.py:161} INFO - Started process (PID=206) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:33:01.624+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:33:01.625+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:33:01.624+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:33:01.646+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:33:01.675+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:33:01.675+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:33:01.695+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:33:01.694+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:33:01.713+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.094 seconds
[2025-12-09T12:33:32.117+0000] {processor.py:161} INFO - Started process (PID=211) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:33:32.118+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:33:32.119+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:33:32.119+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:33:32.136+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:33:32.162+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:33:32.162+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:33:32.180+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:33:32.180+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:33:32.201+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.086 seconds
[2025-12-09T12:34:02.336+0000] {processor.py:161} INFO - Started process (PID=216) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:34:02.338+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:34:02.338+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:34:02.338+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:34:02.355+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:34:02.380+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:34:02.380+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:34:02.399+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:34:02.399+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:34:02.423+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.089 seconds
[2025-12-09T12:34:33.414+0000] {processor.py:161} INFO - Started process (PID=221) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:34:33.415+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:34:33.416+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:34:33.415+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:34:33.432+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:34:33.457+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:34:33.457+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:34:33.476+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:34:33.476+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:34:33.503+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.092 seconds
[2025-12-09T12:35:04.443+0000] {processor.py:161} INFO - Started process (PID=226) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:35:04.446+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:35:04.448+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:35:04.447+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:35:04.488+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:35:04.532+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:35:04.532+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:35:04.565+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:35:04.565+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:35:04.597+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.157 seconds
[2025-12-09T12:35:35.349+0000] {processor.py:161} INFO - Started process (PID=231) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:35:35.351+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:35:35.352+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:35:35.352+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:35:35.388+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:35:35.428+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:35:35.428+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:35:35.451+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:35:35.451+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:35:35.475+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.131 seconds
[2025-12-09T12:36:05.548+0000] {processor.py:161} INFO - Started process (PID=241) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:36:05.549+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:36:05.550+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:36:05.550+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:36:05.574+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:36:05.643+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:36:05.643+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:36:05.663+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:36:05.663+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:36:05.684+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.140 seconds
[2025-12-09T12:36:36.838+0000] {processor.py:161} INFO - Started process (PID=474) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:36:36.849+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:36:36.857+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:36:36.856+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:36:37.070+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:36:37.397+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:36:37.396+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:36:37.593+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:36:37.593+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:36:37.689+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.928 seconds
[2025-12-09T12:37:07.945+0000] {processor.py:161} INFO - Started process (PID=634) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:37:07.953+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:37:07.968+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:37:07.955+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:37:08.628+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:37:37.121+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T12:38:09.607+0000] {processor.py:161} INFO - Started process (PID=648) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:38:10.148+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:38:10.557+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:38:10.364+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:38:17.407+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:39:57.982+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T12:41:33.471+0000] {processor.py:161} INFO - Started process (PID=686) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:41:33.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:41:33.506+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:41:33.498+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:41:34.040+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:41:35.595+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:41:35.593+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:41:35.944+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:41:35.943+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:41:36.116+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.736 seconds
[2025-12-09T12:42:06.727+0000] {processor.py:161} INFO - Started process (PID=734) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:42:06.732+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:42:06.742+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:42:06.736+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:42:07.094+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:42:10.139+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:42:10.129+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:42:07.883+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:42:07.848+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:42:09.877+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 5.828 seconds
[2025-12-09T12:42:40.986+0000] {processor.py:161} INFO - Started process (PID=746) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:42:40.988+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:42:40.992+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:42:40.989+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:42:41.064+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:42:41.915+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:42:41.894+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:42:42.448+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:42:42.433+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:42:42.617+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.638 seconds
[2025-12-09T12:43:12.966+0000] {processor.py:161} INFO - Started process (PID=761) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:43:12.972+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:43:12.975+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:43:12.973+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:43:13.116+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:43:13.967+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:43:13.951+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:43:14.249+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:43:14.242+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:43:14.501+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.545 seconds
[2025-12-09T12:43:45.125+0000] {processor.py:161} INFO - Started process (PID=766) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:43:45.127+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:43:45.131+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:43:45.128+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:43:45.206+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:43:46.417+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:43:46.394+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:43:47.016+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:43:46.994+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:43:48.951+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.831 seconds
[2025-12-09T12:44:19.494+0000] {processor.py:161} INFO - Started process (PID=799) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:44:19.511+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:44:19.527+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:44:19.517+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:44:19.862+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:44:20.956+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:44:20.922+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:44:21.265+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:44:21.249+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:44:21.673+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.227 seconds
[2025-12-09T12:44:52.261+0000] {processor.py:161} INFO - Started process (PID=804) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:44:52.263+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:44:52.266+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:44:52.264+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:44:52.335+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:44:53.477+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:44:53.458+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:44:53.944+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:44:53.924+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:44:54.336+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.083 seconds
[2025-12-09T12:45:25.172+0000] {processor.py:161} INFO - Started process (PID=815) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:45:25.174+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:45:25.178+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:45:25.175+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:45:25.247+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:45:26.188+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:45:26.170+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:45:26.440+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:45:26.408+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:45:27.152+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.988 seconds
[2025-12-09T12:45:58.300+0000] {processor.py:161} INFO - Started process (PID=841) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:45:58.303+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:45:58.307+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:45:58.304+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:45:58.380+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:45:58.959+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:45:58.953+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:45:59.024+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:45:59.024+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:45:59.082+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.790 seconds
[2025-12-09T12:46:29.556+0000] {processor.py:161} INFO - Started process (PID=846) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:46:29.559+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:46:29.564+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:46:29.560+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:46:29.632+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:46:30.164+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:46:30.158+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:46:30.210+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:46:30.210+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:46:30.261+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.712 seconds
[2025-12-09T12:47:00.940+0000] {processor.py:161} INFO - Started process (PID=851) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:47:00.942+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:47:00.945+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:47:00.943+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:47:01.002+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:47:01.329+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:47:01.314+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:47:01.505+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:47:01.499+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:47:01.554+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.625 seconds
[2025-12-09T12:47:32.200+0000] {processor.py:161} INFO - Started process (PID=856) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:47:32.202+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:47:32.206+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:47:32.203+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:47:32.254+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:47:32.428+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:47:32.428+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:47:32.455+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:47:32.455+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:47:32.489+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.295 seconds
[2025-12-09T12:48:03.460+0000] {processor.py:161} INFO - Started process (PID=861) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:48:03.463+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:48:03.466+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:48:03.464+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:48:03.515+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:48:03.657+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:48:03.657+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:48:03.691+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:48:03.691+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:48:03.717+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.269 seconds
[2025-12-09T12:48:34.371+0000] {processor.py:161} INFO - Started process (PID=866) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:48:34.372+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:48:34.376+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:48:34.374+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:48:34.425+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:48:34.514+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:48:34.514+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:48:34.539+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:48:34.538+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:48:34.567+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.203 seconds
[2025-12-09T12:49:05.187+0000] {processor.py:161} INFO - Started process (PID=871) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:49:05.188+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:49:05.191+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:49:05.189+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:49:05.227+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:49:05.295+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:49:05.295+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:49:05.317+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:49:05.316+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:49:05.341+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.161 seconds
[2025-12-09T12:49:36.251+0000] {processor.py:161} INFO - Started process (PID=876) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:49:36.253+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:49:36.255+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:49:36.254+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:49:36.294+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:49:36.377+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:49:36.377+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:49:36.401+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:49:36.400+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:49:36.425+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.180 seconds
[2025-12-09T12:50:07.169+0000] {processor.py:161} INFO - Started process (PID=881) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:50:07.170+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:50:07.172+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:50:07.171+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:50:07.261+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:50:07.341+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:50:07.341+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:50:07.364+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:50:07.364+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:50:07.385+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.223 seconds
[2025-12-09T12:50:38.250+0000] {processor.py:161} INFO - Started process (PID=886) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:50:38.261+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:50:38.272+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:50:38.263+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:50:38.390+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:50:38.506+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:50:38.506+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:50:38.544+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:50:38.544+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:50:38.587+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.367 seconds
[2025-12-09T12:51:09.181+0000] {processor.py:161} INFO - Started process (PID=891) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:51:09.182+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:51:09.185+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:51:09.183+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:51:09.220+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:51:09.296+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:51:09.295+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:51:09.318+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:51:09.318+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:51:09.342+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.169 seconds
[2025-12-09T12:51:39.487+0000] {processor.py:161} INFO - Started process (PID=896) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:51:39.489+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:51:39.492+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:51:39.490+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:51:39.519+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:51:39.599+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:51:39.599+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:51:39.621+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:51:39.621+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:51:39.649+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.169 seconds
[2025-12-09T12:52:09.807+0000] {processor.py:161} INFO - Started process (PID=901) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:52:09.809+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:52:09.812+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:52:09.810+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:52:09.843+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:52:09.930+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:52:09.930+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:52:09.957+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:52:09.956+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:52:09.980+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.179 seconds
[2025-12-09T12:52:40.702+0000] {processor.py:161} INFO - Started process (PID=906) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:52:40.704+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:52:40.707+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:52:40.705+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:52:40.742+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:52:40.827+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:52:40.827+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:52:40.852+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:52:40.852+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:52:40.872+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.175 seconds
[2025-12-09T12:53:11.713+0000] {processor.py:161} INFO - Started process (PID=911) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:53:11.714+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:53:11.717+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:53:11.715+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:53:11.750+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:53:11.830+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:53:11.829+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:53:11.853+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:53:11.852+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:53:11.876+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.169 seconds
[2025-12-09T12:53:42.601+0000] {processor.py:161} INFO - Started process (PID=916) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:53:42.603+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:53:42.605+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:53:42.603+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:53:42.638+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:53:42.730+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:53:42.730+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:53:42.755+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:53:42.755+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:53:42.777+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.182 seconds
[2025-12-09T12:54:13.682+0000] {processor.py:161} INFO - Started process (PID=921) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:54:13.683+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:54:13.686+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:54:13.684+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:54:13.796+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:54:13.876+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:54:13.876+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:54:13.898+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:54:13.898+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:54:13.919+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.244 seconds
[2025-12-09T12:54:44.694+0000] {processor.py:161} INFO - Started process (PID=926) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:54:44.697+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:54:44.701+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:54:44.698+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:54:44.749+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:54:44.863+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:54:44.862+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:54:44.887+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:54:44.886+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:54:44.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.222 seconds
[2025-12-09T12:55:15.696+0000] {processor.py:161} INFO - Started process (PID=931) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:55:15.699+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:55:15.702+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:55:15.700+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:55:15.748+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:55:15.832+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:55:15.832+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:55:15.857+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:55:15.857+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:55:15.880+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.193 seconds
[2025-12-09T12:55:46.729+0000] {processor.py:161} INFO - Started process (PID=936) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:55:46.730+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:55:46.733+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:55:46.731+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:55:46.770+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:55:46.904+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:55:46.904+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:55:46.929+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:55:46.929+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:55:46.952+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.230 seconds
[2025-12-09T12:56:17.929+0000] {processor.py:161} INFO - Started process (PID=941) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:56:17.931+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:56:17.935+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:56:17.932+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:56:17.979+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:56:18.155+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:56:18.154+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:56:18.187+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:56:18.186+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:56:18.214+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.291 seconds
[2025-12-09T12:56:48.895+0000] {processor.py:161} INFO - Started process (PID=946) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:56:48.897+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:56:48.899+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:56:48.898+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:56:48.945+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:56:49.145+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:56:49.138+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:56:49.173+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:56:49.173+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:56:49.204+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.316 seconds
[2025-12-09T12:57:19.685+0000] {processor.py:161} INFO - Started process (PID=951) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:57:19.687+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:57:19.691+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:57:19.688+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:57:19.745+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:57:20.052+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:57:20.047+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:57:20.094+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:57:20.093+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:57:20.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.456 seconds
[2025-12-09T12:57:53.105+0000] {processor.py:161} INFO - Started process (PID=956) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:57:53.134+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:57:53.166+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:57:53.139+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:57:58.761+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:58:00.169+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:58:00.157+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:58:00.649+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:58:00.636+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:58:00.762+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 7.735 seconds
[2025-12-09T12:58:33.766+0000] {processor.py:161} INFO - Started process (PID=961) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:58:33.850+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:58:33.954+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:58:33.908+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:58:34.867+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:59:00.442+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:59:00.423+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:59:00.718+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:59:00.709+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:59:01.475+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 32.110 seconds
[2025-12-09T12:59:31.844+0000] {processor.py:161} INFO - Started process (PID=966) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:59:31.849+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T12:59:31.853+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:59:31.850+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:59:32.014+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T12:59:47.976+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:59:47.857+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T12:59:49.713+0000] {logging_mixin.py:188} INFO - [2025-12-09T12:59:49.707+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T12:59:49.801+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 20.572 seconds
[2025-12-09T13:00:21.976+0000] {processor.py:161} INFO - Started process (PID=971) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:00:22.081+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:00:22.109+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:00:22.094+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:00:22.582+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:00:30.121+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:00:29.996+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:00:33.147+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:00:33.120+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:00:33.256+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 14.122 seconds
[2025-12-09T13:01:04.352+0000] {processor.py:161} INFO - Started process (PID=976) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:01:04.371+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:01:04.381+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:01:04.372+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:01:06.318+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:01:25.066+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:01:25.051+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:01:25.382+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:01:25.369+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:01:26.894+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 25.287 seconds
[2025-12-09T13:01:57.515+0000] {processor.py:161} INFO - Started process (PID=981) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:01:57.519+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:01:57.523+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:01:57.521+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:01:57.625+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:01:58.463+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:01:58.443+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:01:58.731+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:01:58.715+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:02:00.223+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.717 seconds
[2025-12-09T13:02:30.982+0000] {processor.py:161} INFO - Started process (PID=986) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:02:30.986+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:02:30.992+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:02:30.987+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:02:31.092+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:02:38.692+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:02:38.657+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:02:39.179+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:02:39.170+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:02:39.784+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 8.815 seconds
[2025-12-09T13:03:10.444+0000] {processor.py:161} INFO - Started process (PID=991) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:03:10.451+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:03:10.457+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:03:10.452+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:03:10.640+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:03:18.869+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:03:18.790+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:03:23.933+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:03:23.914+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:03:26.688+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 18.939 seconds
[2025-12-09T13:03:57.459+0000] {processor.py:161} INFO - Started process (PID=996) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:03:57.464+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:03:57.469+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:03:57.466+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:03:57.609+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:03:59.492+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:03:59.479+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:03:59.973+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:03:59.957+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:04:01.492+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 4.048 seconds
[2025-12-09T13:04:32.131+0000] {processor.py:161} INFO - Started process (PID=1001) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:04:32.133+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:04:32.136+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:04:32.134+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:04:32.211+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:04:33.148+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:04:33.136+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:04:33.381+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:04:33.369+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:04:33.826+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.702 seconds
[2025-12-09T13:05:04.694+0000] {processor.py:161} INFO - Started process (PID=1006) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:05:04.696+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:05:04.699+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:05:04.697+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:05:04.772+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:05:08.244+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:05:08.180+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:05:11.744+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:05:11.734+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:05:12.064+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 9.889 seconds
[2025-12-09T13:05:42.492+0000] {processor.py:161} INFO - Started process (PID=1011) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:05:42.497+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:05:42.501+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:05:42.498+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:05:42.593+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:05:53.408+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:05:53.366+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:05:58.450+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:05:58.438+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:05:58.598+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 16.120 seconds
[2025-12-09T13:06:29.227+0000] {processor.py:161} INFO - Started process (PID=1016) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:06:29.234+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:06:29.239+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:06:29.236+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:06:29.367+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:06:30.981+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:06:30.967+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:06:31.219+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:06:31.210+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:06:31.496+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.287 seconds
[2025-12-09T13:07:01.592+0000] {processor.py:161} INFO - Started process (PID=1021) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:07:01.593+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:07:01.597+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:07:01.594+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:07:01.663+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:07:04.529+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:07:04.462+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:07:12.677+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:07:12.665+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:07:13.139+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 11.554 seconds
[2025-12-09T13:07:43.666+0000] {processor.py:161} INFO - Started process (PID=1026) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:07:43.667+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:07:43.671+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:07:43.669+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:07:43.723+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:07:44.054+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:07:44.045+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:07:44.189+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:07:44.182+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:07:44.564+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.905 seconds
[2025-12-09T13:08:14.807+0000] {processor.py:161} INFO - Started process (PID=1031) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:08:14.808+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:08:14.811+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:08:14.809+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:08:14.876+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:08:15.751+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:08:15.736+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:08:17.617+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:08:17.453+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:08:23.847+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 9.048 seconds
[2025-12-09T13:08:54.253+0000] {processor.py:161} INFO - Started process (PID=1036) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:08:54.255+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:08:54.258+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:08:54.256+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:08:54.313+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:08:56.464+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:08:56.443+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:09:00.009+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:08:59.992+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:09:02.109+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 7.863 seconds
[2025-12-09T13:09:32.523+0000] {processor.py:161} INFO - Started process (PID=1041) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:09:32.525+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:09:32.528+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:09:32.526+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:09:32.594+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:09:37.104+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:09:36.889+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:09:50.975+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:09:50.966+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:09:51.202+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 21.129 seconds
[2025-12-09T13:10:21.738+0000] {processor.py:161} INFO - Started process (PID=1046) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:10:21.740+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:10:21.744+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:10:21.741+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:10:21.816+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:10:22.922+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:10:22.901+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:10:23.861+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:10:23.849+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:10:26.284+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 4.556 seconds
[2025-12-09T13:10:57.459+0000] {processor.py:161} INFO - Started process (PID=1051) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:10:57.462+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:10:57.466+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:10:57.463+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:10:57.582+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:10:59.740+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:10:59.608+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:11:11.789+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:11:11.775+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:11:12.164+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 16.436 seconds
[2025-12-09T13:11:42.960+0000] {processor.py:161} INFO - Started process (PID=1056) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:11:42.961+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:11:42.964+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:11:42.962+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:11:43.023+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:11:45.752+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:11:45.716+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:11:46.738+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:11:46.629+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:11:48.936+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 5.981 seconds
[2025-12-09T13:12:19.911+0000] {processor.py:161} INFO - Started process (PID=1061) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:12:19.912+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:12:19.919+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:12:19.915+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:12:20.128+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:12:23.184+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:12:23.165+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:12:26.227+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:12:26.218+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:12:28.597+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 8.699 seconds
[2025-12-09T13:12:59.837+0000] {processor.py:161} INFO - Started process (PID=1066) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:12:59.849+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:12:59.862+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:12:59.853+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:13:00.111+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:13:08.899+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:13:08.585+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:13:08.464+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:13:08.441+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:13:08.985+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 11.031 seconds
[2025-12-09T13:13:39.331+0000] {processor.py:161} INFO - Started process (PID=1071) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:13:39.333+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:13:39.338+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:13:39.335+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:13:39.447+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:13:40.448+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:13:40.411+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:13:42.262+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:13:42.250+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:13:43.026+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.706 seconds
[2025-12-09T13:14:13.856+0000] {processor.py:161} INFO - Started process (PID=1076) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:14:13.858+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:14:13.861+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:14:13.859+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:14:13.922+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:14:14.842+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:14:14.786+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:14:17.686+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:14:17.669+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:14:18.886+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 5.039 seconds
[2025-12-09T13:14:49.177+0000] {processor.py:161} INFO - Started process (PID=1081) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:14:49.178+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:14:49.181+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:14:49.179+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:14:49.243+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:14:50.039+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:14:50.025+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:14:57.325+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:14:57.286+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:15:10.946+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 21.750 seconds
[2025-12-09T13:15:40.106+0000] {processor.py:161} INFO - Started process (PID=1086) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:15:40.108+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:15:40.110+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:15:40.108+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:15:40.170+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:15:40.930+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:15:40.908+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:15:41.141+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:15:41.130+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:15:41.246+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.150 seconds
[2025-12-09T13:16:12.154+0000] {processor.py:161} INFO - Started process (PID=1091) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:16:12.177+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:16:12.288+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:16:12.221+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:16:13.211+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:16:31.123+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:16:31.106+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:16:31.355+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:16:31.344+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:16:31.673+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 21.373 seconds
[2025-12-09T13:17:02.143+0000] {processor.py:161} INFO - Started process (PID=1096) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:17:02.146+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:17:02.150+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:17:02.147+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:17:02.281+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:17:10.851+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:17:10.836+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:17:11.145+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:17:11.131+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:17:11.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 9.612 seconds
[2025-12-09T13:17:42.231+0000] {processor.py:161} INFO - Started process (PID=1101) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:17:42.234+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:17:42.237+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:17:42.235+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:17:42.299+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:17:42.836+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:17:42.821+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:17:43.247+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:17:43.224+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:17:44.941+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.721 seconds
[2025-12-09T13:18:15.465+0000] {processor.py:161} INFO - Started process (PID=1106) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:18:15.475+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:18:15.480+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:18:15.476+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:18:15.636+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:18:20.446+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:18:20.435+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:18:24.564+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:18:24.366+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:18:32.516+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 18.815 seconds
[2025-12-09T13:19:02.804+0000] {processor.py:161} INFO - Started process (PID=1111) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:19:02.812+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:19:02.822+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:19:02.813+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:19:04.542+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:19:34.945+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T13:20:05.839+0000] {processor.py:161} INFO - Started process (PID=1116) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:20:05.846+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:20:05.850+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:20:05.847+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:20:05.998+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:20:15.103+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:20:15.090+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:20:15.407+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:20:15.397+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:20:16.712+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 10.888 seconds
[2025-12-09T13:20:47.322+0000] {processor.py:161} INFO - Started process (PID=1121) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:20:47.324+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:20:47.329+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:20:47.326+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:20:47.459+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:20:47.054+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:20:47.042+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:20:47.862+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:20:47.818+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:20:49.138+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.792 seconds
[2025-12-09T13:21:19.562+0000] {processor.py:161} INFO - Started process (PID=1126) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:21:19.575+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:21:19.617+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:21:19.610+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:21:19.860+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:21:20.877+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:21:20.860+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:21:22.477+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:21:22.404+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:21:25.263+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 5.719 seconds
[2025-12-09T13:21:55.742+0000] {processor.py:161} INFO - Started process (PID=1131) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:21:55.745+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:21:55.750+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:21:55.748+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:21:55.858+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:21:56.788+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:21:56.774+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:21:56.881+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:21:56.870+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:21:57.826+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.096 seconds
[2025-12-09T13:22:28.044+0000] {processor.py:161} INFO - Started process (PID=1136) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:22:28.047+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:22:28.050+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:22:28.048+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:22:28.117+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:22:28.707+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:22:28.693+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:22:28.977+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:22:28.968+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:22:29.788+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.756 seconds
[2025-12-09T13:23:00.149+0000] {processor.py:161} INFO - Started process (PID=1141) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:23:00.150+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:23:00.153+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:23:00.151+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:23:00.206+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:23:00.712+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:23:00.693+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:23:00.981+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:23:00.973+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:23:01.389+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.248 seconds
[2025-12-09T13:23:31.593+0000] {processor.py:161} INFO - Started process (PID=1146) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:23:31.596+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:23:31.599+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:23:31.597+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:23:31.754+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:24:00.711+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:24:00.698+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:24:01.129+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:24:01.116+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:24:01.547+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 31.871 seconds
[2025-12-09T13:24:32.019+0000] {processor.py:161} INFO - Started process (PID=1151) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:24:32.021+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:24:32.024+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:24:32.022+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:24:32.085+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:24:33.463+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:24:33.341+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:24:51.689+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:24:51.672+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:24:51.875+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 21.757 seconds
[2025-12-09T13:25:22.445+0000] {processor.py:161} INFO - Started process (PID=1156) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:25:22.448+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:25:22.452+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:25:22.449+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:25:22.601+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:25:24.729+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:25:24.711+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:25:25.016+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:25:24.996+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:25:25.476+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.043 seconds
[2025-12-09T13:25:56.094+0000] {processor.py:161} INFO - Started process (PID=1161) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:25:56.100+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:25:56.104+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:25:56.101+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:25:56.238+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:26:04.068+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:26:04.060+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:26:04.131+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:26:04.130+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:26:04.213+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 8.140 seconds
[2025-12-09T13:26:34.661+0000] {processor.py:161} INFO - Started process (PID=1166) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:26:34.663+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:26:34.666+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:26:34.664+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:26:34.715+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:26:35.124+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:26:35.096+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:26:35.343+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:26:35.334+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:26:36.364+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.711 seconds
[2025-12-09T13:27:07.160+0000] {processor.py:161} INFO - Started process (PID=1171) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:27:07.161+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:27:07.165+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:27:07.162+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:27:07.222+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:27:07.625+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:27:07.604+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:27:07.943+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:27:07.933+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:27:08.764+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.613 seconds
[2025-12-09T13:27:39.018+0000] {processor.py:161} INFO - Started process (PID=1176) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:27:39.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:27:39.022+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:27:39.020+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:27:39.068+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:27:39.429+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:27:39.416+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:27:39.801+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:27:39.720+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:27:41.957+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.944 seconds
[2025-12-09T13:28:12.382+0000] {processor.py:161} INFO - Started process (PID=1181) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:28:12.385+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:28:12.389+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:28:12.387+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:28:12.448+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:28:13.194+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:28:13.178+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:28:15.828+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:28:15.778+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:28:17.951+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 5.575 seconds
[2025-12-09T13:28:49.151+0000] {processor.py:161} INFO - Started process (PID=1186) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:28:49.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:28:49.159+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:28:49.156+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:28:49.275+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:28:52.842+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:28:52.760+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:28:56.922+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:28:56.911+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:28:57.172+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 8.036 seconds
[2025-12-09T13:29:27.979+0000] {processor.py:161} INFO - Started process (PID=1191) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:29:27.981+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:29:27.983+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:29:27.982+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:29:28.036+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:29:28.883+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:29:28.832+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:29:34.601+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:29:34.516+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:29:37.408+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 11.325 seconds
[2025-12-09T13:30:08.347+0000] {processor.py:161} INFO - Started process (PID=1196) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:30:08.349+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:30:08.355+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:30:08.351+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:30:08.464+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:30:12.343+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:30:12.265+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:30:14.372+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:30:14.324+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:30:32.178+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 25.922 seconds
[2025-12-09T13:31:03.469+0000] {processor.py:161} INFO - Started process (PID=1201) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:31:03.471+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:31:03.474+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:31:03.472+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:31:03.532+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:31:04.624+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:31:04.524+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:31:06.867+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:31:06.848+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:31:08.731+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 5.264 seconds
[2025-12-09T13:31:38.583+0000] {processor.py:161} INFO - Started process (PID=1206) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:31:38.618+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:31:38.633+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:31:38.623+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:31:37.683+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:31:51.873+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:31:51.865+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:31:51.960+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:31:51.949+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:31:52.190+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 15.914 seconds
[2025-12-09T13:32:22.832+0000] {processor.py:161} INFO - Started process (PID=1211) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:32:22.858+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:32:22.981+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:32:22.888+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:32:23.744+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:32:53.273+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T13:33:23.707+0000] {processor.py:161} INFO - Started process (PID=1216) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:33:23.759+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:33:23.914+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:33:23.869+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:33:24.475+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:33:35.759+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:33:35.738+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:33:35.843+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:33:35.839+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:33:35.976+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 12.336 seconds
[2025-12-09T13:34:06.923+0000] {processor.py:161} INFO - Started process (PID=1221) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:34:06.927+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:34:06.932+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:34:06.929+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:34:07.139+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:34:11.640+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:34:11.618+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:34:12.233+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:34:12.223+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:34:13.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 8.590 seconds
[2025-12-09T13:34:43.826+0000] {processor.py:161} INFO - Started process (PID=1226) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:34:43.829+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:34:43.833+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:34:43.830+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:34:43.937+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:34:47.915+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:34:47.883+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:34:48.637+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:34:48.617+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:34:50.862+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 7.043 seconds
[2025-12-09T13:41:32.789+0000] {processor.py:161} INFO - Started process (PID=172) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:41:32.790+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:41:32.792+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:41:32.792+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:41:32.833+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:41:32.862+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:41:32.862+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:41:32.914+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:41:32.913+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:41:32.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.181 seconds
[2025-12-09T13:42:03.297+0000] {processor.py:161} INFO - Started process (PID=177) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:42:03.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:42:03.306+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:42:03.305+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:42:03.384+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:42:04.185+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:42:04.184+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:42:04.253+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:42:04.252+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:42:04.291+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.038 seconds
[2025-12-09T13:42:34.695+0000] {processor.py:161} INFO - Started process (PID=190) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:42:34.696+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:42:34.697+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:42:34.697+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:42:34.724+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:42:34.759+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:42:34.759+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:42:34.785+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:42:34.785+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:42:34.821+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.129 seconds
[2025-12-09T13:43:05.262+0000] {processor.py:161} INFO - Started process (PID=195) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:43:05.269+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:43:05.278+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:43:05.275+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:43:05.404+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:43:05.542+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:43:05.541+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:43:05.638+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:43:05.638+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:43:05.684+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.451 seconds
[2025-12-09T13:43:36.128+0000] {processor.py:161} INFO - Started process (PID=211) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:43:36.130+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:43:36.131+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:43:36.131+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:43:36.173+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:43:36.263+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:43:36.263+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:43:36.326+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:43:36.326+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:43:36.362+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.243 seconds
[2025-12-09T13:44:06.937+0000] {processor.py:161} INFO - Started process (PID=471) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:44:06.955+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:44:06.968+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:44:06.966+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:44:07.238+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:44:08.210+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:44:08.209+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:44:09.041+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:44:09.037+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:44:09.205+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.327 seconds
[2025-12-09T13:44:40.010+0000] {processor.py:161} INFO - Started process (PID=606) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:44:40.021+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:44:40.045+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:44:40.036+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:44:40.557+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:44:41.568+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:44:41.565+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:44:42.811+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:44:42.809+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:44:43.356+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.469 seconds
[2025-12-09T13:45:15.626+0000] {processor.py:161} INFO - Started process (PID=668) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:45:15.826+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:45:16.309+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:45:16.062+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:45:50.073+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:45:46.916+0000] {timeout.py:68} ERROR - Process timed out, PID: 668
[2025-12-09T13:45:55.692+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:45:51.461+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/email_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/email_pipeline_dag.py", line 87, in <module>
    aggregate_insights = BashOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 152, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 806, in __init__
    dag = dag or DagContext.get_current_dag()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 4008, in get_current_dag
    return cls._context_managed_dags[0]
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/email_pipeline_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 668
[2025-12-09T13:45:58.696+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:46:39.363+0000] {processor.py:161} INFO - Started process (PID=686) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:46:39.407+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:46:39.915+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:46:39.630+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:46:45.143+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:46:47.794+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:46:47.777+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:46:48.627+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:46:48.605+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:46:49.218+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 10.436 seconds
[2025-12-09T13:47:23.607+0000] {processor.py:161} INFO - Started process (PID=765) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:47:22.476+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:47:25.859+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:47:24.659+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:48:05.272+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:48:00.659+0000] {timeout.py:68} ERROR - Process timed out, PID: 765
[2025-12-09T13:48:17.990+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:48:06.219+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/email_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/email_pipeline_dag.py", line 15, in <module>
    with DAG(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 601, in __init__
    self.timetable = create_timetable(schedule_interval, self.timezone)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 229, in create_timetable
    return CronDataIntervalTimetable(interval, timezone)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 73, in __init__
    descriptor = ExpressionDescriptor(
  File "/home/airflow/.local/lib/python3.8/site-packages/cron_descriptor/ExpressionDescriptor.py", line 72, in __init__
    self.get_text = GetText(options.locale_code, options.locale_location)
  File "/home/airflow/.local/lib/python3.8/site-packages/cron_descriptor/GetText.py", line 48, in __init__
    self.trans = self.load_locale(locale_code, locale_location)
  File "/home/airflow/.local/lib/python3.8/site-packages/cron_descriptor/GetText.py", line 64, in load_locale
    trans = gettext.GNUTranslations(f)
  File "/usr/local/lib/python3.8/gettext.py", line 261, in __init__
    self._parse(fp)
  File "/usr/local/lib/python3.8/gettext.py", line 456, in _parse
    catalog[str(msg, charset)] = str(tmsg, charset)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/email_pipeline_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 765
[2025-12-09T13:48:19.062+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:59:23.602+0000] {processor.py:161} INFO - Started process (PID=770) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:59:23.924+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:59:24.341+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:59:24.232+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:59:27.457+0000] {processor.py:161} INFO - Started process (PID=792) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:59:27.472+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T13:59:27.509+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:59:27.486+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:59:28.222+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T13:59:27.571+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:59:27.569+0000] {taskinstance.py:2700} ERROR - {'DAG Id': 'email_intelligence_pipeline', 'Task Id': 'quality_assurance_gate', 'Run Id': 'manual__2025-12-09T13:43:25.927565+00:00', 'Hostname': '202880cbc844'}
[2025-12-09T13:59:27.910+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:59:27.910+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=email_intelligence_pipeline, task_id=quality_assurance_gate, execution_date=20251209T134325, start_date=20251209T134357, end_date=20251209T135927
[2025-12-09T13:59:28.028+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: email_intelligence_pipeline.quality_assurance_gate manual__2025-12-09T13:43:25.927565+00:00 [failed]> in state failed
[2025-12-09T13:59:30.619+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:59:30.617+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T13:59:30.803+0000] {logging_mixin.py:188} INFO - [2025-12-09T13:59:30.802+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T13:59:30.928+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 5.088 seconds
[2025-12-09T14:00:01.261+0000] {processor.py:161} INFO - Started process (PID=1032) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:00:01.270+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:00:01.290+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:00:01.278+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:00:01.497+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:00:02.077+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:00:02.069+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:00:02.243+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:00:02.242+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:00:02.402+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.193 seconds
[2025-12-09T14:03:59.177+0000] {processor.py:161} INFO - Started process (PID=1059) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:03:59.222+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:03:59.307+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:03:59.271+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:04:00.885+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:04:03.728+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:04:03.725+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:04:04.164+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:04:04.161+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:04:04.348+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 7.184 seconds
[2025-12-09T14:04:36.054+0000] {processor.py:161} INFO - Started process (PID=1154) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:04:36.426+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:04:37.601+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:04:36.577+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:06:18.338+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:05:30.926+0000] {timeout.py:68} ERROR - Process timed out, PID: 1154
[2025-12-09T14:07:08.054+0000] {processor.py:161} INFO - Started process (PID=1166) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:07:08.150+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:07:08.383+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:07:08.215+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:07:15.557+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:09:10.324+0000] {processor.py:161} INFO - Started process (PID=1169) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:09:10.669+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:09:11.576+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:09:11.403+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:09:13.724+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:09:43.107+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T14:10:15.996+0000] {processor.py:161} INFO - Started process (PID=1175) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:10:16.131+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:10:16.935+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:10:16.556+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:10:49.934+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:10:46.772+0000] {timeout.py:68} ERROR - Process timed out, PID: 1175
[2025-12-09T14:11:12.304+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:10:54.341+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/email_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/email_pipeline_dag.py", line 62, in <module>
    quality_gate = BashOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 434, in apply_defaults
    self._BaseOperator__init_kwargs = {}
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 1045, in __setattr__
    super().__setattr__(key, value)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/email_pipeline_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 1175
[2025-12-09T14:11:21.975+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:19:11.943+0000] {processor.py:161} INFO - Started process (PID=1178) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:19:12.066+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:19:12.241+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:19:12.120+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:19:15.410+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:19:32.770+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:19:32.758+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:19:33.120+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:19:33.117+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:19:33.317+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 24.406 seconds
[2025-12-09T14:19:36.330+0000] {processor.py:161} INFO - Started process (PID=1215) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:19:36.340+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:19:36.353+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:19:36.347+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:19:36.816+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:19:36.909+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:19:36.907+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:19:37.156+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:19:37.155+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:19:37.327+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.025 seconds
[2025-12-09T14:20:08.163+0000] {processor.py:161} INFO - Started process (PID=1228) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:20:08.167+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:20:08.175+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:20:08.169+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:20:08.260+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:20:09.778+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:20:09.750+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:20:09.892+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:20:09.891+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:20:10.075+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.933 seconds
[2025-12-09T14:20:40.550+0000] {processor.py:161} INFO - Started process (PID=1233) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:20:40.553+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:20:40.563+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:20:40.555+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:20:40.786+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:20:42.193+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:20:42.166+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:20:42.765+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:20:42.743+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:23:56.469+0000] {processor.py:161} INFO - Started process (PID=1246) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:23:56.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:23:56.482+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:23:56.476+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:23:56.568+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:24:03.630+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:24:03.629+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:24:03.712+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:24:03.710+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:24:03.814+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 9.338 seconds
[2025-12-09T14:24:35.519+0000] {processor.py:161} INFO - Started process (PID=1261) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:24:35.527+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:24:35.537+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:24:35.531+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:24:35.674+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:24:36.695+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:24:36.681+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:24:36.853+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:24:36.836+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:24:36.987+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.487 seconds
[2025-12-09T14:25:07.851+0000] {processor.py:161} INFO - Started process (PID=1266) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:25:07.863+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:25:07.872+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:25:07.866+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:25:07.983+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:25:09.324+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:25:09.306+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:25:09.874+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:25:09.857+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:25:10.195+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.370 seconds
[2025-12-09T14:25:41.584+0000] {processor.py:161} INFO - Started process (PID=1301) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:25:41.589+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:25:41.599+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:25:41.594+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:25:41.685+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:25:42.731+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:25:42.695+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:25:43.163+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:25:43.135+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:25:43.429+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.863 seconds
[2025-12-09T14:26:13.892+0000] {processor.py:161} INFO - Started process (PID=1307) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:26:13.902+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:26:13.912+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:26:13.905+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:26:14.114+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:26:22.659+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:26:22.632+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:26:22.843+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:26:22.827+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:26:23.354+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 9.506 seconds
[2025-12-09T14:26:54.129+0000] {processor.py:161} INFO - Started process (PID=1312) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:26:54.169+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:26:54.197+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:26:54.180+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:26:54.441+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:26:56.483+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:26:56.458+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:26:56.942+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:26:56.914+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:26:57.316+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 5.619 seconds
[2025-12-09T14:27:28.163+0000] {processor.py:161} INFO - Started process (PID=1317) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:27:28.168+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:27:28.176+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:27:28.172+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:27:28.363+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:27:38.082+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:27:38.035+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:27:38.754+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:27:38.731+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:27:39.362+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 11.217 seconds
[2025-12-09T14:28:10.298+0000] {processor.py:161} INFO - Started process (PID=1343) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:28:10.312+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:28:10.321+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:28:10.315+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:28:10.474+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:28:15.718+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:28:15.682+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:28:16.299+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:28:16.273+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:28:17.288+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 7.023 seconds
[2025-12-09T14:28:48.627+0000] {processor.py:161} INFO - Started process (PID=1348) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:28:48.631+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:28:48.646+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:28:48.637+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:28:48.787+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:28:51.835+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:28:51.798+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:28:52.123+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:28:52.109+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:28:52.590+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.980 seconds
[2025-12-09T14:29:23.049+0000] {processor.py:161} INFO - Started process (PID=1353) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:29:23.060+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:29:23.073+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:29:23.067+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:29:23.333+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:29:25.219+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:29:25.198+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:29:25.637+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:29:25.605+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:29:26.003+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 5.191 seconds
[2025-12-09T14:29:57.425+0000] {processor.py:161} INFO - Started process (PID=1358) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:29:57.440+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:29:57.455+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:29:57.446+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:29:57.582+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:29:59.636+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:29:59.611+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:30:00.186+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:30:00.163+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:30:00.813+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.422 seconds
[2025-12-09T14:30:31.148+0000] {processor.py:161} INFO - Started process (PID=1363) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:30:31.157+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:30:31.167+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:30:31.162+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:30:31.275+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:30:36.190+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:30:36.160+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:30:36.721+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:30:36.706+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:30:37.127+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 5.996 seconds
[2025-12-09T14:31:08.045+0000] {processor.py:161} INFO - Started process (PID=1368) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:31:08.051+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:31:08.058+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:31:08.054+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:31:08.252+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:31:09.456+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:31:09.430+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:31:10.080+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:31:10.060+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:31:10.403+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.378 seconds
[2025-12-09T14:31:41.679+0000] {processor.py:161} INFO - Started process (PID=1383) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:31:41.682+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:31:41.692+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:31:41.684+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:31:41.873+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:31:42.690+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:31:42.670+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:31:42.902+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:31:42.889+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:31:43.629+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.966 seconds
[2025-12-09T14:32:14.593+0000] {processor.py:161} INFO - Started process (PID=1388) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:32:14.600+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:32:14.611+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:32:14.603+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:32:14.764+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:32:16.151+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:32:16.123+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:32:16.636+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:32:16.616+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:32:17.413+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.854 seconds
[2025-12-09T14:32:48.742+0000] {processor.py:161} INFO - Started process (PID=1393) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:32:48.750+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:32:48.755+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:32:48.751+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:32:48.830+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:32:51.174+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:32:51.126+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:32:51.384+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:32:51.370+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:32:51.588+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.897 seconds
[2025-12-09T14:33:22.367+0000] {processor.py:161} INFO - Started process (PID=1398) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:33:22.379+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:33:22.403+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:33:22.390+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:33:22.687+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:33:23.394+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:33:23.362+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:33:23.717+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:33:23.687+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:33:24.573+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 4.503 seconds
[2025-12-09T14:33:54.982+0000] {processor.py:161} INFO - Started process (PID=1403) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:33:54.988+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:33:55.001+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:33:54.993+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:33:55.077+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:33:53.705+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:33:53.694+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:33:53.954+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:33:53.939+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:33:54.165+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.516 seconds
[2025-12-09T14:34:24.802+0000] {processor.py:161} INFO - Started process (PID=1408) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:34:24.813+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:34:24.821+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:34:24.817+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:34:24.946+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:34:24.542+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:34:24.498+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:34:24.827+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:34:24.806+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:34:26.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.608 seconds
[2025-12-09T14:34:56.850+0000] {processor.py:161} INFO - Started process (PID=1413) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:34:56.858+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:34:56.863+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:34:56.860+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:34:56.929+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:34:57.965+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:34:57.950+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:34:58.035+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:34:58.034+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:34:58.202+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.362 seconds
[2025-12-09T14:35:29.311+0000] {processor.py:161} INFO - Started process (PID=1418) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:35:29.319+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:35:29.329+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:35:29.322+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:35:29.564+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:35:36.347+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:35:36.315+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:35:36.691+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:35:36.679+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:35:36.858+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 7.578 seconds
[2025-12-09T14:36:07.626+0000] {processor.py:161} INFO - Started process (PID=1423) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:36:07.631+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:36:07.643+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:36:07.636+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:36:07.734+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:36:09.397+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:36:09.351+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:36:09.832+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:36:09.818+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:36:10.052+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.440 seconds
[2025-12-09T14:36:40.609+0000] {processor.py:161} INFO - Started process (PID=1428) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:36:40.621+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:36:40.633+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:36:40.628+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:36:40.741+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:36:49.757+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:36:49.713+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:36:50.672+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:36:50.653+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:36:52.255+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 11.671 seconds
[2025-12-09T14:37:23.233+0000] {processor.py:161} INFO - Started process (PID=1433) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:37:23.235+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:37:23.239+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:37:23.236+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:37:23.350+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:37:24.172+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:37:24.162+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:37:24.427+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:37:24.418+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:37:22.399+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.503 seconds
[2025-12-09T14:37:52.772+0000] {processor.py:161} INFO - Started process (PID=1438) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:37:52.784+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:37:52.797+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:37:52.789+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:37:52.949+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:37:54.215+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:37:54.166+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:37:52.535+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:37:52.509+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:37:52.764+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.331 seconds
[2025-12-09T14:38:23.339+0000] {processor.py:161} INFO - Started process (PID=1443) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:38:23.361+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:38:23.389+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:38:23.377+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:38:23.641+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:38:24.987+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:38:24.961+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:38:25.335+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:38:25.299+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:38:25.723+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 4.768 seconds
[2025-12-09T14:38:56.826+0000] {processor.py:161} INFO - Started process (PID=1448) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:38:56.831+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:38:56.837+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:38:56.834+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:38:56.989+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:38:58.889+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:38:58.871+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:38:59.645+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:38:59.622+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:39:00.147+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.336 seconds
[2025-12-09T14:39:30.525+0000] {processor.py:161} INFO - Started process (PID=1453) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:39:30.535+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:39:30.546+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:39:30.540+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:39:30.732+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:39:31.752+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:39:31.728+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:39:32.149+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:39:32.125+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:39:32.550+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.061 seconds
[2025-12-09T14:40:03.028+0000] {processor.py:161} INFO - Started process (PID=1458) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:40:03.033+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:40:03.042+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:40:03.036+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:40:03.193+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:40:05.988+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:40:05.966+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:40:06.539+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:40:06.512+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:40:07.086+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 4.070 seconds
[2025-12-09T14:40:37.604+0000] {processor.py:161} INFO - Started process (PID=1463) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:40:37.616+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:40:37.636+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:40:37.625+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:40:37.796+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:40:38.369+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:40:38.366+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:40:38.536+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:40:38.531+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:40:38.654+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.095 seconds
[2025-12-09T14:41:09.080+0000] {processor.py:161} INFO - Started process (PID=1468) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:41:09.085+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:41:09.094+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:41:09.089+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:41:09.212+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:41:09.710+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:41:09.700+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:41:09.802+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:41:09.800+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:41:09.893+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.829 seconds
[2025-12-09T14:41:40.665+0000] {processor.py:161} INFO - Started process (PID=1473) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:41:40.671+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:41:40.684+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:41:40.676+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:41:40.802+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:41:41.573+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:41:41.559+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:41:41.648+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:41:41.647+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:41:41.732+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.099 seconds
[2025-12-09T14:42:12.494+0000] {processor.py:161} INFO - Started process (PID=1478) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:42:12.501+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:42:12.520+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:42:12.508+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:42:12.607+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:42:13.470+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:42:13.449+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:42:13.614+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:42:13.613+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:42:13.728+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.267 seconds
[2025-12-09T14:42:44.598+0000] {processor.py:161} INFO - Started process (PID=1483) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:42:44.604+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:42:44.612+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:42:44.608+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:42:44.740+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:42:46.356+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:42:46.315+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:42:46.856+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:42:46.833+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:42:47.954+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.377 seconds
[2025-12-09T14:43:18.333+0000] {processor.py:161} INFO - Started process (PID=1488) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:43:18.342+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:43:18.358+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:43:18.347+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:43:18.471+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:43:19.683+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:43:19.653+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:43:20.248+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:43:20.226+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:43:21.116+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.803 seconds
[2025-12-09T14:43:51.982+0000] {processor.py:161} INFO - Started process (PID=1493) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:43:51.984+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:43:51.987+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:43:51.985+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:43:52.051+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:43:52.451+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:43:52.448+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:43:52.549+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:43:52.549+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:43:52.653+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.682 seconds
[2025-12-09T14:44:23.172+0000] {processor.py:161} INFO - Started process (PID=1498) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:44:23.180+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:44:23.195+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:44:23.190+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:44:23.301+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:44:23.630+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:44:23.629+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:44:23.748+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:44:23.747+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:44:23.829+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.688 seconds
[2025-12-09T14:44:54.591+0000] {processor.py:161} INFO - Started process (PID=1503) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:44:54.594+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:44:54.600+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:44:54.595+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:44:54.687+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:44:55.384+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:44:55.369+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:44:55.509+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:44:55.508+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:44:55.586+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.012 seconds
[2025-12-09T14:45:26.482+0000] {processor.py:161} INFO - Started process (PID=1508) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:45:26.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:45:26.496+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:45:26.490+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:45:26.633+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:45:31.984+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:45:31.962+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:45:32.201+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:45:32.181+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:45:33.061+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 9.083 seconds
[2025-12-09T14:46:04.153+0000] {processor.py:161} INFO - Started process (PID=1513) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:46:04.164+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:46:04.174+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:46:04.169+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:46:04.287+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:46:04.892+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:46:04.869+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:46:04.986+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:46:04.986+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:46:05.126+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.995 seconds
[2025-12-09T14:46:36.106+0000] {processor.py:161} INFO - Started process (PID=1518) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:46:36.113+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:46:36.130+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:46:36.119+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:46:36.259+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:47:05.160+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T14:47:36.381+0000] {processor.py:161} INFO - Started process (PID=1523) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:47:36.389+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:47:36.413+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:47:36.404+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:47:36.523+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:47:37.578+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:47:37.569+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:47:38.011+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:47:37.979+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:47:38.547+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.188 seconds
[2025-12-09T14:48:09.732+0000] {processor.py:161} INFO - Started process (PID=1528) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:48:09.742+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:48:09.759+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:48:09.747+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:48:09.823+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:48:10.507+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:48:10.475+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:48:10.752+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:48:10.738+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:48:10.959+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.254 seconds
[2025-12-09T14:48:41.789+0000] {processor.py:161} INFO - Started process (PID=1533) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:48:41.792+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:48:41.802+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:48:41.794+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:48:41.911+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:48:42.970+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:48:42.949+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:48:43.266+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:48:43.246+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:48:43.588+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.829 seconds
[2025-12-09T14:49:14.508+0000] {processor.py:161} INFO - Started process (PID=1538) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:49:14.512+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:49:14.518+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:49:14.514+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:49:14.588+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:49:15.043+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:49:15.026+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:49:15.152+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:49:15.151+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:49:15.368+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.876 seconds
[2025-12-09T14:49:46.080+0000] {processor.py:161} INFO - Started process (PID=1543) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:49:46.082+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:49:46.086+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:49:46.083+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:49:46.197+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:49:48.131+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:49:48.116+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:49:48.231+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:49:48.230+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:49:48.388+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.323 seconds
[2025-12-09T14:50:19.268+0000] {processor.py:161} INFO - Started process (PID=1548) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:50:19.273+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:50:19.278+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:50:19.274+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:50:19.387+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:50:20.248+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:50:20.233+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:50:20.466+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:50:20.440+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:50:21.004+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.764 seconds
[2025-12-09T14:50:51.492+0000] {processor.py:161} INFO - Started process (PID=1553) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:50:51.496+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:50:51.511+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:50:51.502+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:50:51.570+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:50:52.460+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:50:52.445+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:50:52.634+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:50:52.627+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:50:52.939+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.466 seconds
[2025-12-09T14:51:23.481+0000] {processor.py:161} INFO - Started process (PID=1558) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:51:23.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:51:23.506+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:51:23.497+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:51:23.737+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:51:25.417+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:51:25.386+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:51:25.919+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:51:25.891+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:51:26.181+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.733 seconds
[2025-12-09T14:51:56.503+0000] {processor.py:161} INFO - Started process (PID=1563) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:51:56.510+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:51:56.520+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:51:56.514+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:51:56.618+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:51:57.549+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:51:57.530+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:51:57.828+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:51:57.807+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:51:58.198+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.707 seconds
[2025-12-09T14:52:29.499+0000] {processor.py:161} INFO - Started process (PID=1568) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:52:29.506+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:52:29.515+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:52:29.511+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:52:29.619+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:52:30.923+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:52:30.889+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:52:31.159+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:52:31.131+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:52:31.431+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.949 seconds
[2025-12-09T14:53:01.834+0000] {processor.py:161} INFO - Started process (PID=1573) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:53:01.840+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:53:01.852+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:53:01.844+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:53:01.966+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:53:02.902+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:53:02.882+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:53:03.068+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:53:03.050+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:53:03.244+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.432 seconds
[2025-12-09T14:53:34.477+0000] {processor.py:161} INFO - Started process (PID=1578) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:53:34.480+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:53:34.484+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:53:34.481+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:53:34.548+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:53:35.098+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:53:35.089+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:53:35.295+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:53:35.286+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:53:35.436+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.976 seconds
[2025-12-09T14:54:06.001+0000] {processor.py:161} INFO - Started process (PID=1583) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:54:06.006+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:54:06.013+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:54:06.009+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:54:06.200+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:54:12.062+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:54:12.034+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:54:12.869+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:54:12.859+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:54:14.100+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 8.113 seconds
[2025-12-09T14:54:45.214+0000] {processor.py:161} INFO - Started process (PID=1588) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:54:45.225+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:54:45.246+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:54:45.232+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:54:45.440+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:54:46.576+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:54:46.548+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:54:46.952+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:54:46.941+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:54:47.221+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.053 seconds
[2025-12-09T14:55:17.875+0000] {processor.py:161} INFO - Started process (PID=1593) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:55:17.880+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:55:17.888+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:55:17.883+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:55:17.975+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:55:18.755+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:55:18.733+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:55:18.922+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:55:18.921+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:55:19.120+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.262 seconds
[2025-12-09T14:55:50.311+0000] {processor.py:161} INFO - Started process (PID=1598) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:55:50.317+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:55:50.328+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:55:50.319+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:55:50.400+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:55:51.169+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:55:51.150+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:55:51.317+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:55:51.316+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:55:51.446+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.159 seconds
[2025-12-09T14:56:21.858+0000] {processor.py:161} INFO - Started process (PID=1624) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:56:21.863+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:56:21.879+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:56:21.869+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:56:22.085+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:56:23.223+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:56:23.175+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:56:23.704+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:56:23.675+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:56:25.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.279 seconds
[2025-12-09T14:56:55.939+0000] {processor.py:161} INFO - Started process (PID=1629) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:56:55.945+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:56:55.953+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:56:55.948+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:56:56.052+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:56:56.769+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:56:56.740+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:56:56.989+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:56:56.957+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:56:57.176+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.254 seconds
[2025-12-09T14:57:27.475+0000] {processor.py:161} INFO - Started process (PID=1634) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:57:27.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:57:27.499+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:57:27.493+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:57:27.640+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:57:29.473+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:57:29.423+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:57:29.990+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:57:29.962+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:57:30.435+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.987 seconds
[2025-12-09T14:58:01.280+0000] {processor.py:161} INFO - Started process (PID=1639) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:58:01.288+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:58:01.294+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:58:01.290+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:58:01.366+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:58:02.184+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:58:02.169+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:58:02.276+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:58:02.275+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:58:02.495+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.245 seconds
[2025-12-09T14:58:32.850+0000] {processor.py:161} INFO - Started process (PID=1644) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:58:32.853+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:58:32.859+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:58:32.854+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:58:33.012+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:58:36.068+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:58:36.031+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:58:36.290+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:58:36.272+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:58:36.768+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.946 seconds
[2025-12-09T14:59:07.111+0000] {processor.py:161} INFO - Started process (PID=1649) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:59:07.124+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:59:07.137+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:59:07.130+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:59:07.271+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:59:08.191+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:59:08.173+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:59:08.354+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:59:08.345+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:59:08.751+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.657 seconds
[2025-12-09T14:59:40.017+0000] {processor.py:161} INFO - Started process (PID=1654) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:59:40.028+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T14:59:40.044+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:59:40.035+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:59:40.140+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T14:59:41.502+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:59:41.480+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T14:59:42.161+0000] {logging_mixin.py:188} INFO - [2025-12-09T14:59:42.130+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T14:59:42.393+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.402 seconds
[2025-12-09T15:00:13.337+0000] {processor.py:161} INFO - Started process (PID=1659) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:00:13.340+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:00:13.351+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:00:13.345+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:00:13.487+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:00:16.768+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:00:16.750+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:00:17.035+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:00:17.005+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:00:18.781+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 5.461 seconds
[2025-12-09T15:00:49.748+0000] {processor.py:161} INFO - Started process (PID=1664) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:00:49.753+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:00:49.758+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:00:49.755+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:00:49.880+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:00:52.071+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:00:52.042+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:00:52.620+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:00:52.599+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:00:52.890+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.154 seconds
[2025-12-09T15:01:24.129+0000] {processor.py:161} INFO - Started process (PID=1669) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:01:24.135+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:01:24.143+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:01:24.139+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:01:24.249+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:01:26.815+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:01:26.776+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:01:27.310+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:01:27.301+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:01:28.478+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 4.363 seconds
[2025-12-09T15:02:00.162+0000] {processor.py:161} INFO - Started process (PID=1674) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:02:00.166+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:02:00.179+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:02:00.169+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:02:00.268+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:02:01.392+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:02:01.357+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:02:01.590+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:02:01.572+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:02:01.740+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.595 seconds
[2025-12-09T15:02:32.174+0000] {processor.py:161} INFO - Started process (PID=1679) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:02:32.177+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:02:32.183+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:02:32.179+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:02:32.259+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:02:33.199+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:02:33.170+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:02:33.793+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:02:33.770+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:02:34.227+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.069 seconds
[2025-12-09T15:03:05.262+0000] {processor.py:161} INFO - Started process (PID=1684) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:03:05.274+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:03:05.288+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:03:05.280+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:03:05.462+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:03:09.880+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:03:09.856+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:03:10.245+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:03:10.224+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:03:11.293+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 6.052 seconds
[2025-12-09T15:03:41.699+0000] {processor.py:161} INFO - Started process (PID=1689) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:03:41.703+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:03:41.706+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:03:41.704+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:03:41.846+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:03:45.282+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:03:45.270+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:03:45.572+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:03:45.553+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:03:46.496+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 4.814 seconds
[2025-12-09T15:04:16.767+0000] {processor.py:161} INFO - Started process (PID=1694) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:04:16.773+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:04:16.788+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:04:16.780+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:04:16.945+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:04:20.216+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:04:20.189+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:04:21.044+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:04:21.008+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:04:21.652+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 4.910 seconds
[2025-12-09T15:04:52.539+0000] {processor.py:161} INFO - Started process (PID=1699) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:04:52.542+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:04:52.551+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:04:52.546+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:04:52.665+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:05:07.219+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:05:07.201+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:05:07.319+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:05:07.311+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:05:07.595+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 16.946 seconds
[2025-12-09T15:05:37.933+0000] {processor.py:161} INFO - Started process (PID=1704) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:05:37.938+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:05:37.943+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:05:37.940+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:05:38.043+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:05:39.148+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:05:39.124+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:05:40.010+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:05:39.991+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:05:40.436+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.515 seconds
[2025-12-09T15:06:10.883+0000] {processor.py:161} INFO - Started process (PID=1709) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:06:10.885+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:06:10.899+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:06:10.890+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:06:11.004+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:06:12.308+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:06:12.275+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:06:12.539+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:06:12.521+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:06:12.682+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.831 seconds
[2025-12-09T15:06:43.164+0000] {processor.py:161} INFO - Started process (PID=1714) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:06:43.170+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:06:43.180+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:06:43.174+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:06:43.251+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:06:43.930+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:06:43.913+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:06:44.121+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:06:44.106+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:06:44.300+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.161 seconds
[2025-12-09T15:07:14.609+0000] {processor.py:161} INFO - Started process (PID=1719) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:07:14.612+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:07:14.619+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:07:14.614+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:07:14.723+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:07:15.614+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:07:15.591+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:07:15.803+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:07:15.795+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:07:16.012+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.416 seconds
[2025-12-09T15:07:46.314+0000] {processor.py:161} INFO - Started process (PID=1724) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:07:46.320+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:07:46.331+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:07:46.326+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:07:46.451+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:07:47.789+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:07:47.745+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:07:48.662+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:07:48.631+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:07:49.398+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.096 seconds
[2025-12-09T15:08:20.301+0000] {processor.py:161} INFO - Started process (PID=1729) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:08:20.305+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:08:20.311+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:08:20.308+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:08:20.419+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:08:21.314+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:08:21.289+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:08:22.048+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:08:22.029+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:08:22.222+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.936 seconds
[2025-12-09T15:08:53.259+0000] {processor.py:161} INFO - Started process (PID=1734) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:08:53.267+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:08:53.275+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:08:53.269+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:08:53.385+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:08:55.185+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:08:55.154+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:08:55.344+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:08:55.332+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:08:56.471+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 5.258 seconds
[2025-12-09T15:09:27.440+0000] {processor.py:161} INFO - Started process (PID=1739) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:09:27.443+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:09:27.453+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:09:27.445+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:09:27.518+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:09:28.665+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:09:28.612+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:09:29.350+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:09:29.330+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:09:29.649+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.224 seconds
[2025-12-09T15:10:00.871+0000] {processor.py:161} INFO - Started process (PID=1744) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:10:00.878+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:10:00.896+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:10:00.882+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:10:01.093+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:10:30.282+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T15:11:01.019+0000] {processor.py:161} INFO - Started process (PID=1749) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:11:01.022+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:11:01.026+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:11:01.023+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:11:01.154+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:11:09.189+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:11:09.169+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:11:09.516+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:11:09.490+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:11:10.148+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 9.144 seconds
[2025-12-09T15:11:40.790+0000] {processor.py:161} INFO - Started process (PID=1754) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:11:40.801+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:11:40.815+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:11:40.806+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:11:41.005+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:11:44.587+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:11:44.537+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:11:44.780+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:11:44.758+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:11:45.344+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 4.577 seconds
[2025-12-09T15:12:16.065+0000] {processor.py:161} INFO - Started process (PID=1759) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:12:16.070+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:12:16.076+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:12:16.072+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:12:16.225+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:12:17.724+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:12:17.695+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:12:17.912+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:12:17.887+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:12:19.117+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.075 seconds
[2025-12-09T15:12:50.328+0000] {processor.py:161} INFO - Started process (PID=1764) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:12:50.331+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:12:50.339+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:12:50.335+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:12:50.466+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:12:51.405+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:12:51.376+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:12:52.017+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:12:51.992+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:12:52.378+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.076 seconds
[2025-12-09T15:13:23.257+0000] {processor.py:161} INFO - Started process (PID=1769) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:13:23.259+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:13:23.266+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:13:23.261+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:13:23.402+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:13:25.857+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:13:25.821+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:13:26.579+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:13:26.564+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:13:26.858+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.621 seconds
[2025-12-09T15:13:57.806+0000] {processor.py:161} INFO - Started process (PID=1774) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:13:57.810+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:13:57.816+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:13:57.811+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:13:57.967+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:13:59.432+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:13:59.395+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:13:59.992+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:13:59.977+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:14:01.103+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.318 seconds
[2025-12-09T15:14:31.849+0000] {processor.py:161} INFO - Started process (PID=1779) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:14:31.856+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:14:31.876+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:14:31.867+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:14:32.008+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:14:33.276+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:14:33.233+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:14:33.535+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:14:33.521+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:14:33.942+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.119 seconds
[2025-12-09T15:15:04.926+0000] {processor.py:161} INFO - Started process (PID=1784) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:15:04.944+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:15:04.962+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:15:04.948+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:15:05.164+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:15:05.786+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:15:05.778+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:15:05.934+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:15:05.931+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:15:06.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.341 seconds
[2025-12-09T15:15:36.870+0000] {processor.py:161} INFO - Started process (PID=1789) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:15:36.879+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:15:36.892+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:15:36.884+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:15:36.968+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:15:37.956+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:15:37.930+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:15:38.262+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:15:38.244+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:15:38.625+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.785 seconds
[2025-12-09T15:16:09.590+0000] {processor.py:161} INFO - Started process (PID=1794) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:16:09.603+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:16:09.615+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:16:09.608+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:16:09.841+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:16:11.248+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:16:11.214+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:16:11.953+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:16:11.838+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:16:13.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.599 seconds
[2025-12-09T15:16:44.299+0000] {processor.py:161} INFO - Started process (PID=1799) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:16:44.304+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:16:44.310+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:16:44.306+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:16:44.399+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:16:46.581+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:16:46.566+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:16:46.958+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:16:46.945+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:16:48.107+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.821 seconds
[2025-12-09T15:17:19.112+0000] {processor.py:161} INFO - Started process (PID=1804) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:17:19.125+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:17:19.132+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:17:19.126+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:17:19.234+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:17:27.660+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:17:27.609+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:17:26.892+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:17:26.866+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:17:27.921+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 10.826 seconds
[2025-12-09T15:17:58.283+0000] {processor.py:161} INFO - Started process (PID=1809) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:17:58.285+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:17:58.289+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:17:58.286+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:17:58.327+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:17:58.651+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:17:58.643+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:17:58.706+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:17:58.701+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:17:58.846+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.573 seconds
[2025-12-09T15:18:27.812+0000] {processor.py:161} INFO - Started process (PID=1814) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:18:27.819+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:18:27.823+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:18:27.820+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:18:27.884+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:18:31.392+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:18:31.375+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:18:31.784+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:18:31.773+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:18:32.473+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 6.860 seconds
[2025-12-09T15:19:03.368+0000] {processor.py:161} INFO - Started process (PID=1819) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:19:03.369+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:19:03.373+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:19:03.370+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:19:03.418+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:19:03.867+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:19:03.848+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:19:04.188+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:19:04.168+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:19:05.429+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.072 seconds
[2025-12-09T15:19:35.803+0000] {processor.py:161} INFO - Started process (PID=1824) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:19:35.805+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:19:35.808+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:19:35.806+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:19:35.834+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:19:36.201+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:19:36.190+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:19:36.250+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:19:36.250+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:19:36.301+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.506 seconds
[2025-12-09T15:20:07.404+0000] {processor.py:161} INFO - Started process (PID=1829) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:20:07.407+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:20:07.410+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:20:07.408+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:20:07.442+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:20:08.068+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:20:08.055+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:20:08.166+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:20:08.155+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:20:08.411+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.015 seconds
[2025-12-09T15:20:39.139+0000] {processor.py:161} INFO - Started process (PID=1834) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:20:39.142+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:20:39.146+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:20:39.143+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:20:39.185+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:20:39.875+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:20:39.859+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:20:40.024+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:20:40.014+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:20:40.360+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.231 seconds
[2025-12-09T15:21:10.989+0000] {processor.py:161} INFO - Started process (PID=1839) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:21:10.991+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:21:10.995+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:21:10.992+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:21:11.038+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:21:12.375+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:21:12.351+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:21:13.861+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:21:13.759+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:21:15.862+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 4.886 seconds
[2025-12-09T15:21:48.264+0000] {processor.py:161} INFO - Started process (PID=1844) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:21:48.712+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:21:49.072+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:21:48.923+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:21:50.830+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:21:55.063+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:21:54.946+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:21:59.549+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:21:59.541+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:21:59.845+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 11.847 seconds
[2025-12-09T15:22:30.290+0000] {processor.py:161} INFO - Started process (PID=1849) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:22:30.315+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:22:30.330+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:22:30.319+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:22:30.741+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:22:41.534+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:22:41.501+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:22:47.514+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:22:47.486+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:22:54.639+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 24.400 seconds
[2025-12-09T15:23:25.375+0000] {processor.py:161} INFO - Started process (PID=1854) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:23:25.384+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:23:25.388+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:23:25.385+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:23:25.442+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:23:28.062+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:23:27.946+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:23:29.295+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:23:29.279+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:23:30.744+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 6.628 seconds
[2025-12-09T15:36:55.840+0000] {processor.py:161} INFO - Started process (PID=1859) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:36:55.843+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:36:55.856+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:36:55.850+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:36:55.961+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:36:56.967+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:36:56.959+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:36:57.113+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:36:57.111+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:36:57.750+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.937 seconds
[2025-12-09T15:37:28.110+0000] {processor.py:161} INFO - Started process (PID=1866) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:37:28.113+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:37:28.121+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:37:28.116+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:37:28.211+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:37:28.388+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:37:28.387+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:37:28.414+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:37:28.414+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:37:28.447+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.353 seconds
[2025-12-09T15:37:59.438+0000] {processor.py:161} INFO - Started process (PID=1871) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:37:59.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:37:59.446+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:37:59.444+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:37:59.510+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:37:59.681+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:37:59.681+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:37:59.731+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:37:59.731+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:37:59.778+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.349 seconds
[2025-12-09T15:38:30.158+0000] {processor.py:161} INFO - Started process (PID=1876) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:38:30.161+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:38:30.165+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:38:30.162+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:38:30.223+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:38:30.374+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:38:30.373+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:38:30.405+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:38:30.405+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:38:30.444+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.297 seconds
[2025-12-09T15:39:01.463+0000] {processor.py:161} INFO - Started process (PID=1881) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:39:01.468+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:39:01.475+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:39:01.469+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:39:01.553+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:39:02.302+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:39:02.284+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:39:02.442+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:39:02.430+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:39:02.949+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.499 seconds
[2025-12-09T15:39:33.970+0000] {processor.py:161} INFO - Started process (PID=1886) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:39:33.972+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:39:33.977+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:39:33.974+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:39:34.012+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:39:34.438+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:39:34.428+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:39:34.473+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:39:34.472+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:39:34.513+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.552 seconds
[2025-12-09T15:40:05.364+0000] {processor.py:161} INFO - Started process (PID=1891) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:40:05.366+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:40:05.370+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:40:05.368+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:40:05.408+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:40:05.870+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:40:05.859+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:40:05.919+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:40:05.919+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:40:05.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.612 seconds
[2025-12-09T15:40:36.576+0000] {processor.py:161} INFO - Started process (PID=1896) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:40:36.579+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:40:36.583+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:40:36.580+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:40:36.630+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:40:36.873+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:40:36.872+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:40:36.908+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:40:36.908+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:40:36.949+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.382 seconds
[2025-12-09T15:41:07.767+0000] {processor.py:161} INFO - Started process (PID=1901) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:41:07.770+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:41:07.775+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:41:07.772+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:41:07.822+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:41:08.090+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:41:08.088+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:41:08.122+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:41:08.121+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:41:08.185+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.428 seconds
[2025-12-09T15:41:38.696+0000] {processor.py:161} INFO - Started process (PID=1906) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:41:38.697+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:41:38.700+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:41:38.698+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:41:38.746+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:41:38.956+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:41:38.955+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:41:38.983+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:41:38.983+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:41:39.018+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.333 seconds
[2025-12-09T15:42:10.177+0000] {processor.py:161} INFO - Started process (PID=1911) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:42:10.180+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:42:10.187+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:42:10.183+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:42:10.271+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:42:10.560+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:42:10.559+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:42:10.624+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:42:10.623+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:42:10.668+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.505 seconds
[2025-12-09T15:42:41.323+0000] {processor.py:161} INFO - Started process (PID=1916) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:42:41.325+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:42:41.329+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:42:41.326+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:42:41.377+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:42:41.593+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:42:41.593+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:42:41.620+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:42:41.620+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:42:41.649+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.336 seconds
[2025-12-09T15:43:12.389+0000] {processor.py:161} INFO - Started process (PID=1921) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:43:12.390+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:43:12.394+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:43:12.391+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:43:12.424+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:43:12.625+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:43:12.625+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:43:12.656+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:43:12.656+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:43:12.690+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.309 seconds
[2025-12-09T15:43:43.017+0000] {processor.py:161} INFO - Started process (PID=1926) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:43:43.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:43:43.029+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:43:43.024+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:43:43.088+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:43:43.458+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:43:43.457+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:43:43.531+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:43:43.531+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:43:43.582+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.577 seconds
[2025-12-09T15:44:14.794+0000] {processor.py:161} INFO - Started process (PID=1931) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:44:14.802+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:44:14.809+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:44:14.805+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:44:14.906+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:44:15.563+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:44:15.551+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:44:15.673+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:44:15.671+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:44:15.806+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.031 seconds
[2025-12-09T15:44:46.459+0000] {processor.py:161} INFO - Started process (PID=1936) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:44:46.462+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:44:46.466+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:44:46.463+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:44:46.533+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:44:46.794+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:44:46.794+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:44:46.832+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:44:46.832+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:44:46.881+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.431 seconds
[2025-12-09T15:45:17.202+0000] {processor.py:161} INFO - Started process (PID=1941) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:45:17.207+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:45:17.212+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:45:17.208+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:45:17.295+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:45:17.567+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:45:17.566+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:45:17.646+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:45:17.646+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:45:17.706+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.524 seconds
[2025-12-09T15:45:47.966+0000] {processor.py:161} INFO - Started process (PID=1946) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:45:47.968+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:45:47.971+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:45:47.969+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:45:48.026+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:45:48.325+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:45:48.324+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:45:48.351+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:45:48.350+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:45:48.383+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.432 seconds
[2025-12-09T15:46:18.706+0000] {processor.py:161} INFO - Started process (PID=1951) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:46:18.708+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:46:18.711+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:46:18.709+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:46:18.743+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:46:18.931+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:46:18.931+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:46:18.967+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:46:18.966+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:46:19.002+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.303 seconds
[2025-12-09T15:46:49.304+0000] {processor.py:161} INFO - Started process (PID=1956) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:46:49.306+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:46:49.310+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:46:49.307+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:46:49.359+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:46:49.605+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:46:49.605+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:46:49.633+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:46:49.633+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:46:49.658+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.362 seconds
[2025-12-09T15:47:20.412+0000] {processor.py:161} INFO - Started process (PID=1961) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:47:20.415+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:47:20.419+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:47:20.417+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:47:20.448+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:47:20.603+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:47:20.603+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:47:20.633+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:47:20.633+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:47:20.666+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.260 seconds
[2025-12-09T15:47:50.858+0000] {processor.py:161} INFO - Started process (PID=1966) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:47:50.860+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:47:50.866+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:47:50.862+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:47:50.908+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:47:51.185+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:47:51.177+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:47:51.212+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:47:51.212+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:47:51.250+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.402 seconds
[2025-12-09T15:48:21.867+0000] {processor.py:161} INFO - Started process (PID=1971) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:48:21.869+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:48:21.873+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:48:21.870+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:48:21.901+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:48:22.071+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:48:22.070+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:48:22.098+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:48:22.097+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:48:22.125+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.266 seconds
[2025-12-09T15:48:52.495+0000] {processor.py:161} INFO - Started process (PID=1976) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:48:52.497+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:48:52.503+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:48:52.500+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:48:52.552+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:48:54.246+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:48:54.239+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:48:54.279+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:48:54.278+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:48:54.314+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.829 seconds
[2025-12-09T15:49:24.808+0000] {processor.py:161} INFO - Started process (PID=1981) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:49:24.810+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:49:24.814+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:49:24.811+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:49:24.855+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:49:25.062+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:49:25.061+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:49:25.085+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:49:25.085+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:49:25.113+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.314 seconds
[2025-12-09T15:49:56.064+0000] {processor.py:161} INFO - Started process (PID=1986) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:49:56.066+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:49:56.069+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:49:56.067+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:49:56.099+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:49:56.405+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:49:56.397+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:49:56.451+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:49:56.450+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:49:56.515+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.460 seconds
[2025-12-09T15:50:27.322+0000] {processor.py:161} INFO - Started process (PID=1991) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:50:27.324+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:50:27.329+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:50:27.326+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:50:27.367+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:50:25.795+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:50:25.786+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:50:25.846+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:50:25.845+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:50:25.884+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.469 seconds
[2025-12-09T15:50:56.097+0000] {processor.py:161} INFO - Started process (PID=1996) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:50:56.099+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:50:56.103+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:50:56.100+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:50:56.154+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:50:56.472+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:50:56.463+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:50:56.550+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:50:56.548+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:50:56.596+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.508 seconds
[2025-12-09T15:51:27.101+0000] {processor.py:161} INFO - Started process (PID=2001) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:51:27.103+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:51:27.106+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:51:27.104+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:51:27.172+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:51:27.393+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:51:27.392+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:51:27.419+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:51:27.419+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:51:27.449+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.358 seconds
[2025-12-09T15:51:58.262+0000] {processor.py:161} INFO - Started process (PID=2006) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:51:58.264+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:51:58.268+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:51:58.265+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:51:58.300+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:51:58.591+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:51:58.586+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:51:58.667+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:51:58.661+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:51:58.721+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.467 seconds
[2025-12-09T15:52:29.347+0000] {processor.py:161} INFO - Started process (PID=2011) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:52:29.350+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:52:29.353+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:52:29.351+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:52:29.382+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:52:29.600+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:52:29.594+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:52:29.629+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:52:29.628+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:52:29.664+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.322 seconds
[2025-12-09T15:52:59.823+0000] {processor.py:161} INFO - Started process (PID=2016) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:52:59.825+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:52:59.828+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:52:59.826+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:52:59.855+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:52:59.960+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:52:59.960+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:52:59.986+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:52:59.986+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:53:00.015+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.199 seconds
[2025-12-09T15:53:30.449+0000] {processor.py:161} INFO - Started process (PID=2021) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:53:30.451+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:53:30.455+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:53:30.453+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:53:30.497+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:53:30.668+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:53:30.668+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:53:30.698+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:53:30.697+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:53:30.722+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.281 seconds
[2025-12-09T15:54:01.782+0000] {processor.py:161} INFO - Started process (PID=2026) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:54:01.786+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:54:01.788+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:54:01.786+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:54:01.821+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:54:02.136+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:54:02.131+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:54:02.177+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:54:02.177+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:54:02.229+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.456 seconds
[2025-12-09T15:54:32.973+0000] {processor.py:161} INFO - Started process (PID=2031) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:54:32.975+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:54:32.979+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:54:32.976+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:54:33.026+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:54:33.288+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:54:33.287+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:54:33.311+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:54:33.310+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:54:33.335+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.372 seconds
[2025-12-09T15:55:03.893+0000] {processor.py:161} INFO - Started process (PID=2036) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:55:03.924+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:55:03.930+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:55:03.926+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:55:04.122+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:55:04.818+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:55:04.795+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:55:05.136+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:55:05.111+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:55:05.351+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.477 seconds
[2025-12-09T15:55:35.645+0000] {processor.py:161} INFO - Started process (PID=2041) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:55:35.647+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:55:35.650+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:55:35.648+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:55:35.691+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:55:36.045+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:55:36.037+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:55:36.101+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:55:36.099+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:55:36.166+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.529 seconds
[2025-12-09T15:56:07.175+0000] {processor.py:161} INFO - Started process (PID=2046) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:56:07.176+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:56:07.181+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:56:07.178+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:56:07.221+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:56:07.550+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:56:07.544+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:56:07.590+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:56:07.589+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:56:07.624+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.458 seconds
[2025-12-09T15:56:37.840+0000] {processor.py:161} INFO - Started process (PID=2051) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:56:37.844+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:56:37.850+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:56:37.846+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:56:37.928+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:56:38.536+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:56:38.519+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:56:38.612+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:56:38.611+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:56:39.048+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.219 seconds
[2025-12-09T15:57:09.310+0000] {processor.py:161} INFO - Started process (PID=2056) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:57:09.313+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:57:09.317+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:57:09.314+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:57:09.354+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:57:09.759+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:57:09.751+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:57:09.810+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:57:09.810+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:57:09.856+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.554 seconds
[2025-12-09T15:57:40.744+0000] {processor.py:161} INFO - Started process (PID=2061) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:57:40.746+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:57:40.750+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:57:40.747+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:57:40.808+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:57:41.025+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:57:41.017+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:57:41.058+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:57:41.058+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:57:41.086+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.352 seconds
[2025-12-09T15:58:11.331+0000] {processor.py:161} INFO - Started process (PID=2066) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:58:11.334+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:58:11.336+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:58:11.335+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:58:11.370+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:58:11.608+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:58:11.603+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:58:11.642+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:58:11.642+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:58:11.668+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.344 seconds
[2025-12-09T15:58:42.277+0000] {processor.py:161} INFO - Started process (PID=2071) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:58:42.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:58:42.283+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:58:42.280+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:58:42.319+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:58:42.558+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:58:42.551+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:58:42.599+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:58:42.599+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:58:42.629+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.361 seconds
[2025-12-09T15:59:13.017+0000] {processor.py:161} INFO - Started process (PID=2076) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:59:13.018+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:59:13.021+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:59:13.019+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:59:13.048+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:59:13.419+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:59:13.410+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:59:13.893+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:59:13.864+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:59:14.666+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.654 seconds
[2025-12-09T15:59:45.452+0000] {processor.py:161} INFO - Started process (PID=2081) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:59:45.454+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T15:59:45.456+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:59:45.455+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:59:45.491+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T15:59:45.644+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:59:45.644+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T15:59:45.666+0000] {logging_mixin.py:188} INFO - [2025-12-09T15:59:45.666+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T15:59:45.685+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.241 seconds
[2025-12-09T16:00:16.657+0000] {processor.py:161} INFO - Started process (PID=2086) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:00:16.660+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T16:00:16.664+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:00:16.661+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:00:16.701+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:00:17.055+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:00:17.044+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T16:00:17.124+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:00:17.118+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T16:00:17.246+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.597 seconds
[2025-12-09T16:00:47.419+0000] {processor.py:161} INFO - Started process (PID=2091) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:00:47.421+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T16:00:47.424+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:00:47.422+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:00:47.456+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:00:47.674+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:00:47.667+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T16:00:47.703+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:00:47.703+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T16:00:47.745+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.335 seconds
[2025-12-09T16:01:17.869+0000] {processor.py:161} INFO - Started process (PID=2096) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:01:17.889+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T16:01:17.901+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:01:17.891+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:01:18.193+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:01:47.835+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T16:02:18.304+0000] {processor.py:161} INFO - Started process (PID=2101) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:02:18.309+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T16:02:18.312+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:02:18.310+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:02:18.359+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:02:29.060+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:02:29.001+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T16:02:30.140+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:02:30.126+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T16:02:38.800+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 22.532 seconds
[2025-12-09T16:03:10.916+0000] {processor.py:161} INFO - Started process (PID=2106) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:03:10.918+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T16:03:10.921+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:03:10.919+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:03:10.956+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:03:11.366+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:03:11.352+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T16:03:11.776+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:03:11.765+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T16:03:12.661+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.754 seconds
[2025-12-09T16:03:43.420+0000] {processor.py:161} INFO - Started process (PID=2111) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:03:43.421+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T16:03:43.424+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:03:43.422+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:03:43.480+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:03:43.810+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:03:43.800+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T16:03:43.844+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:03:43.843+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T16:03:43.866+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.453 seconds
[2025-12-09T16:04:14.115+0000] {processor.py:161} INFO - Started process (PID=2116) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:04:14.118+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T16:04:14.121+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:04:14.118+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:04:14.168+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:04:14.541+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:04:14.533+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T16:04:14.774+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:04:14.767+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T16:04:15.582+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.474 seconds
[2025-12-09T16:04:46.498+0000] {processor.py:161} INFO - Started process (PID=2121) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:04:46.499+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T16:04:46.503+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:04:46.500+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:04:46.554+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:04:46.781+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:04:46.781+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T16:04:46.805+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:04:46.805+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T16:04:46.860+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.369 seconds
[2025-12-09T16:05:17.268+0000] {processor.py:161} INFO - Started process (PID=2126) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:05:17.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T16:05:17.284+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:05:17.281+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:05:17.392+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:05:18.489+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:05:18.473+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T16:05:19.032+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:05:18.987+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T16:05:26.734+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 9.482 seconds
[2025-12-09T16:05:57.481+0000] {processor.py:161} INFO - Started process (PID=2131) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:05:57.483+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T16:05:57.486+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:05:57.484+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:05:57.539+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:05:59.793+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:05:59.780+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T16:06:00.424+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:06:00.406+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T16:06:02.060+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 6.663 seconds
[2025-12-09T16:09:17.378+0000] {processor.py:161} INFO - Started process (PID=2138) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:09:17.387+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T16:09:17.400+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:09:17.391+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:09:17.602+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T16:09:19.015+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:09:18.992+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T16:09:19.283+0000] {logging_mixin.py:188} INFO - [2025-12-09T16:09:19.271+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T16:09:19.571+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.257 seconds
[2025-12-09T17:00:19.663+0000] {processor.py:161} INFO - Started process (PID=2146) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:00:20.039+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:00:20.565+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:00:20.112+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:00:20.651+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:00:21.018+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:00:21.010+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:00:21.236+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:00:21.228+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:00:21.553+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.027 seconds
[2025-12-09T17:00:51.936+0000] {processor.py:161} INFO - Started process (PID=2151) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:00:51.948+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:00:51.961+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:00:51.953+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:00:52.171+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:00:54.002+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:00:53.975+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:00:54.671+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:00:54.639+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:00:55.523+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.619 seconds
[2025-12-09T17:01:26.069+0000] {processor.py:161} INFO - Started process (PID=2156) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:01:26.079+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:01:26.095+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:01:26.083+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:01:26.237+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:01:27.142+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:01:27.132+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:01:27.322+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:01:27.293+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:01:27.715+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.685 seconds
[2025-12-09T17:01:58.047+0000] {processor.py:161} INFO - Started process (PID=2161) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:01:58.049+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:01:58.052+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:01:58.050+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:01:58.079+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:01:58.862+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:01:58.845+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:01:59.115+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:01:59.099+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:01:59.876+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.834 seconds
[2025-12-09T17:02:30.698+0000] {processor.py:161} INFO - Started process (PID=2166) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:02:30.714+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:02:30.726+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:02:30.716+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:02:30.880+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:03:00.009+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T17:03:30.436+0000] {processor.py:161} INFO - Started process (PID=2171) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:03:30.454+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:03:30.490+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:03:30.461+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:03:30.671+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:04:07.091+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:04:07.013+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:04:12.118+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:04:12.078+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:04:15.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 48.957 seconds
[2025-12-09T17:04:48.038+0000] {processor.py:161} INFO - Started process (PID=2176) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:04:48.064+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:04:48.090+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:04:48.077+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:04:47.105+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:04:55.951+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:04:55.907+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:04:56.658+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:04:56.644+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:04:57.661+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 11.591 seconds
[2025-12-09T17:05:28.394+0000] {processor.py:161} INFO - Started process (PID=2181) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:05:28.396+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:05:28.402+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:05:28.398+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:05:28.511+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:05:29.205+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:05:29.190+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:05:29.492+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:05:29.474+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:05:30.423+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.044 seconds
[2025-12-09T17:06:00.907+0000] {processor.py:161} INFO - Started process (PID=2186) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:06:01.257+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:06:01.839+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:06:01.731+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:06:02.252+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:06:18.407+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:06:18.399+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:06:18.482+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:06:18.481+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:06:18.556+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 17.692 seconds
[2025-12-09T17:06:48.937+0000] {processor.py:161} INFO - Started process (PID=2191) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:06:48.942+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:06:48.945+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:06:48.943+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:06:48.992+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:06:50.155+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:06:50.128+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:06:49.553+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:06:49.541+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:06:49.656+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.694 seconds
[2025-12-09T17:07:20.554+0000] {processor.py:161} INFO - Started process (PID=2196) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:07:20.557+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:07:20.560+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:07:20.558+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:07:20.620+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:07:20.757+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:07:20.737+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:07:23.222+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:07:23.182+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:07:24.831+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 6.329 seconds
[2025-12-09T17:08:35.806+0000] {processor.py:161} INFO - Started process (PID=2203) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:08:35.902+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:08:35.975+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:08:35.925+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:08:36.532+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:08:38.681+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:08:38.656+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:08:38.907+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:08:38.895+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:08:40.019+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 4.260 seconds
[2025-12-09T17:09:10.491+0000] {processor.py:161} INFO - Started process (PID=2208) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:09:10.499+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:09:10.504+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:09:10.500+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:09:10.601+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:09:39.648+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:09:39.642+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:09:39.684+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:09:39.684+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:09:39.712+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 29.243 seconds
[2025-12-09T17:10:10.387+0000] {processor.py:161} INFO - Started process (PID=2213) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:10:10.728+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:10:12.551+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:10:12.475+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:10:12.352+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:10:21.520+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:10:21.465+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:10:31.829+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:10:31.797+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:10:32.416+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 23.668 seconds
[2025-12-09T17:11:02.645+0000] {processor.py:161} INFO - Started process (PID=2218) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:11:02.648+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:11:02.652+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:11:02.649+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:11:02.733+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:11:11.575+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:11:11.520+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:11:14.282+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:11:14.250+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:11:15.804+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 15.374 seconds
[2025-12-09T17:11:46.555+0000] {processor.py:161} INFO - Started process (PID=2223) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:11:46.559+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:11:46.563+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:11:46.560+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:11:46.612+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:11:47.933+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:11:47.911+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:11:48.505+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:11:48.488+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:11:49.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.590 seconds
[2025-12-09T17:12:20.549+0000] {processor.py:161} INFO - Started process (PID=2228) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:12:20.576+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:12:20.608+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:12:20.586+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:12:20.908+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:12:30.308+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:12:30.245+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:12:30.760+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:12:30.733+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:12:32.230+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 11.812 seconds
[2025-12-09T17:13:02.689+0000] {processor.py:161} INFO - Started process (PID=2233) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:13:02.755+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:13:02.878+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:13:02.812+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:13:03.356+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:13:25.259+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:13:25.249+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:13:26.220+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:13:26.210+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:13:30.207+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 29.451 seconds
[2025-12-09T17:14:01.083+0000] {processor.py:161} INFO - Started process (PID=2238) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:14:01.086+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:14:01.092+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:14:01.087+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:14:01.169+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:14:03.396+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:14:03.363+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:14:03.798+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:14:03.777+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:14:04.678+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.612 seconds
[2025-12-09T17:14:35.138+0000] {processor.py:161} INFO - Started process (PID=2243) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:14:35.144+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:14:35.150+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:14:35.146+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:14:35.249+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:15:04.571+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T17:15:35.383+0000] {processor.py:161} INFO - Started process (PID=2248) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:15:35.390+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:15:35.397+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:15:35.391+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:15:35.612+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:15:45.911+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:15:45.899+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:15:45.956+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:15:45.955+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:15:46.028+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 12.518 seconds
[2025-12-09T17:16:16.331+0000] {processor.py:161} INFO - Started process (PID=2253) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:16:16.334+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:16:16.338+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:16:16.335+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:16:16.402+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:16:18.372+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:16:18.346+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:16:18.517+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:16:18.500+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:16:17.702+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.242 seconds
[2025-12-09T17:16:48.504+0000] {processor.py:161} INFO - Started process (PID=2258) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:16:48.506+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:16:48.509+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:16:48.507+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:16:48.551+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:16:49.282+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:16:49.267+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:16:47.720+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:16:47.710+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:16:47.884+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.193 seconds
[2025-12-09T17:17:18.549+0000] {processor.py:161} INFO - Started process (PID=2263) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:17:18.557+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:17:18.563+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:17:18.559+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:17:18.669+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:17:21.473+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:17:21.434+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:17:22.776+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:17:22.746+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:17:24.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 7.390 seconds
[2025-12-09T17:17:55.085+0000] {processor.py:161} INFO - Started process (PID=2268) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:17:55.087+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:17:55.091+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:17:55.087+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:17:55.142+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:17:55.879+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:17:55.858+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:17:56.122+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:17:56.102+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:17:56.799+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.729 seconds
[2025-12-09T17:18:27.645+0000] {processor.py:161} INFO - Started process (PID=2273) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:18:27.647+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:18:27.650+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:18:27.648+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:18:27.700+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:18:28.838+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:18:28.818+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:18:29.285+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:18:29.261+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:18:29.768+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.135 seconds
[2025-12-09T17:19:01.070+0000] {processor.py:161} INFO - Started process (PID=2278) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:19:01.084+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:19:01.093+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:19:01.086+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:19:01.357+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:19:30.426+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T17:20:02.091+0000] {processor.py:161} INFO - Started process (PID=2283) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:20:02.094+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:20:02.100+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:20:02.096+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:20:02.215+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:20:06.446+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:20:06.426+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:20:06.708+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:20:06.697+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:20:08.289+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 6.215 seconds
[2025-12-09T17:20:39.650+0000] {processor.py:161} INFO - Started process (PID=2288) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:20:39.661+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:20:39.667+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:20:39.663+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:20:39.827+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:20:41.243+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:20:41.224+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:20:41.646+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:20:41.632+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:20:41.844+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.214 seconds
[2025-12-09T17:21:12.228+0000] {processor.py:161} INFO - Started process (PID=2293) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:21:12.236+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:21:12.243+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:21:12.237+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:21:12.335+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:21:17.534+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:21:17.182+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:21:18.999+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:21:18.830+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:21:22.945+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 12.334 seconds
[2025-12-09T17:21:54.482+0000] {processor.py:161} INFO - Started process (PID=2298) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:21:54.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:21:54.493+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:21:54.489+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:21:54.578+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:21:55.851+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:21:55.824+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:21:56.558+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:21:56.537+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:21:58.030+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.565 seconds
[2025-12-09T17:22:28.686+0000] {processor.py:161} INFO - Started process (PID=2303) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:22:28.688+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:22:28.692+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:22:28.689+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:22:28.752+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:22:29.979+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:22:29.951+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:22:32.173+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:22:32.145+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:22:35.202+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 6.526 seconds
[2025-12-09T17:23:06.209+0000] {processor.py:161} INFO - Started process (PID=2308) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:23:06.212+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:23:06.216+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:23:06.213+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:23:06.296+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:23:10.181+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:23:10.151+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:23:12.718+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:23:12.648+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:23:16.953+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 10.758 seconds
[2025-12-09T17:23:48.279+0000] {processor.py:161} INFO - Started process (PID=2313) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:23:49.103+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:23:49.197+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:23:49.124+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:23:55.256+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:24:02.705+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:24:02.688+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:24:02.842+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:24:02.812+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:24:03.683+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 17.376 seconds
[2025-12-09T17:24:34.020+0000] {processor.py:161} INFO - Started process (PID=2318) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:24:34.038+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:24:34.052+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:24:34.042+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:24:34.229+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:25:03.528+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T17:25:33.859+0000] {processor.py:161} INFO - Started process (PID=2323) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:25:33.863+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:25:33.869+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:25:33.865+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:25:33.928+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:25:35.074+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:25:35.055+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:25:35.688+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:25:35.656+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:25:36.535+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.691 seconds
[2025-12-09T17:26:06.948+0000] {processor.py:161} INFO - Started process (PID=2328) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:26:06.954+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:26:06.961+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:26:06.956+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:26:07.132+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:26:36.825+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T17:27:07.624+0000] {processor.py:161} INFO - Started process (PID=2333) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:27:07.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:27:07.635+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:27:07.631+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:27:07.706+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:27:08.805+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:27:08.793+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:27:08.883+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:27:08.883+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:27:08.973+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.360 seconds
[2025-12-09T17:27:42.030+0000] {processor.py:161} INFO - Started process (PID=2338) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:27:42.043+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:27:42.062+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:27:42.045+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:27:46.985+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:28:07.201+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:28:07.190+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:28:07.289+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:28:07.278+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:28:07.440+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 28.240 seconds
[2025-12-09T17:28:38.050+0000] {processor.py:161} INFO - Started process (PID=2343) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:28:38.065+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:28:38.081+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:28:38.068+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:28:38.390+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:29:08.069+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T17:29:39.001+0000] {processor.py:161} INFO - Started process (PID=2348) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:29:39.009+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:29:39.014+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:29:39.010+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:29:39.086+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:29:45.770+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:29:45.745+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:29:48.012+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:29:48.003+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:29:50.235+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 13.162 seconds
[2025-12-09T17:30:21.499+0000] {processor.py:161} INFO - Started process (PID=2353) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:30:21.516+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:30:21.526+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:30:21.517+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:30:21.710+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:34:55.644+0000] {processor.py:161} INFO - Started process (PID=171) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:34:55.646+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:34:55.648+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:34:55.648+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:34:55.700+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:34:55.949+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:34:55.948+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:34:55.971+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:34:55.971+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:34:55.999+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.361 seconds
[2025-12-09T17:35:26.335+0000] {processor.py:161} INFO - Started process (PID=187) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:35:26.338+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:35:26.340+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:35:26.339+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:35:26.403+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:35:26.441+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:35:26.441+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:35:26.470+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:35:26.470+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:35:26.507+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.177 seconds
[2025-12-09T17:35:56.928+0000] {processor.py:161} INFO - Started process (PID=336) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:35:56.935+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:35:56.941+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:35:56.940+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:35:57.124+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:35:57.351+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:35:57.351+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:35:57.483+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:35:57.483+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:35:57.584+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.669 seconds
[2025-12-09T17:36:28.040+0000] {processor.py:161} INFO - Started process (PID=341) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:36:28.046+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:36:28.047+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:36:28.047+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:36:28.101+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:36:28.181+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:36:28.181+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:36:28.289+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:36:28.288+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:36:28.328+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.321 seconds
[2025-12-09T17:36:58.947+0000] {processor.py:161} INFO - Started process (PID=496) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:36:58.955+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:36:58.958+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:36:58.957+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:36:59.061+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:36:59.473+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:36:59.473+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:36:59.634+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:36:59.633+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:36:59.696+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.770 seconds
[2025-12-09T17:37:26.613+0000] {processor.py:161} INFO - Started process (PID=533) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:37:26.626+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:37:26.648+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:37:26.634+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:37:26.821+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:37:27.204+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:37:27.190+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:37:27.267+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:37:27.266+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:37:27.366+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.809 seconds
[2025-12-09T17:37:35.943+0000] {processor.py:161} INFO - Started process (PID=538) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:37:35.948+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:37:35.952+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:37:35.949+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:37:36.115+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:37:52.766+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:37:52.750+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:37:53.105+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:37:53.086+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:37:53.540+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 19.948 seconds
[2025-12-09T17:37:57.604+0000] {processor.py:161} INFO - Started process (PID=543) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:37:57.607+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:37:57.612+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:37:57.608+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:37:57.770+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:37:58.458+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:37:58.447+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:37:58.529+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:37:58.527+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:37:58.608+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.014 seconds
[2025-12-09T17:38:02.907+0000] {processor.py:161} INFO - Started process (PID=548) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:38:02.908+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:38:02.912+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:38:02.909+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:38:03.013+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:38:04.880+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:38:04.859+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:38:05.307+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:38:05.292+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:38:06.128+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.228 seconds
[2025-12-09T17:38:37.704+0000] {processor.py:161} INFO - Started process (PID=555) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:38:37.742+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:38:37.942+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:38:37.854+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:38:40.728+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:39:10.690+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T17:39:42.549+0000] {processor.py:161} INFO - Started process (PID=561) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:39:42.613+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:39:42.648+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:39:42.639+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:39:50.273+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:40:23.897+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:40:23.883+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:40:24.668+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:40:24.643+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:40:24.963+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 44.715 seconds
[2025-12-09T17:40:55.412+0000] {processor.py:161} INFO - Started process (PID=567) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:40:55.418+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:40:55.431+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:40:55.423+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:40:55.729+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:41:26.507+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T17:44:57.872+0000] {processor.py:161} INFO - Started process (PID=169) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:44:57.874+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:44:57.877+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:44:57.876+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:44:57.973+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:44:58.262+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:44:58.262+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:44:58.401+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:44:58.401+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:44:58.498+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.640 seconds
[2025-12-09T17:45:29.028+0000] {processor.py:161} INFO - Started process (PID=183) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:45:29.032+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:45:29.048+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:45:29.042+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:45:29.228+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:45:29.960+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:45:29.959+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:45:30.037+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:45:30.037+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:45:30.083+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.089 seconds
[2025-12-09T17:47:08.057+0000] {processor.py:161} INFO - Started process (PID=188) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:47:08.166+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:47:48.103+0000] {processor.py:161} INFO - Started process (PID=192) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:47:48.161+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:47:48.258+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:47:48.237+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:48:31.332+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:49:53.836+0000] {processor.py:161} INFO - Started process (PID=202) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:49:53.897+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:49:53.924+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:49:53.919+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:49:54.354+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:49:56.619+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:49:56.618+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:49:56.781+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:49:56.780+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:49:56.876+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.250 seconds
[2025-12-09T17:50:28.047+0000] {processor.py:161} INFO - Started process (PID=347) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:50:28.067+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:50:28.090+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:50:28.079+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:50:28.656+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:50:29.807+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:50:29.806+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:50:30.318+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:50:30.317+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:50:30.971+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.037 seconds
[2025-12-09T17:51:03.283+0000] {processor.py:161} INFO - Started process (PID=655) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:51:03.300+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:51:03.354+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:51:03.331+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:51:04.290+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:51:05.497+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:51:05.495+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:51:06.353+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:51:06.350+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:51:06.649+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 4.174 seconds
[2025-12-09T17:59:01.570+0000] {processor.py:161} INFO - Started process (PID=169) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:59:01.572+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:59:01.577+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:59:01.577+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:59:01.612+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:59:01.678+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:59:01.677+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:59:01.722+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:59:01.721+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:59:01.766+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.201 seconds
[2025-12-09T17:59:32.640+0000] {processor.py:161} INFO - Started process (PID=744) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:59:32.657+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T17:59:32.681+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:59:32.672+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:59:33.258+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T17:59:35.277+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:59:35.276+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T17:59:34.242+0000] {logging_mixin.py:188} INFO - [2025-12-09T17:59:34.241+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T17:59:34.295+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.549 seconds
[2025-12-09T18:00:04.660+0000] {processor.py:161} INFO - Started process (PID=1239) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:00:04.662+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:00:04.665+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:00:04.665+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:00:04.720+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:00:04.764+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:00:04.764+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:00:04.789+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:00:04.789+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:00:04.813+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.162 seconds
[2025-12-09T18:00:34.866+0000] {processor.py:161} INFO - Started process (PID=1244) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:00:34.869+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:00:34.870+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:00:34.870+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:00:34.906+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:00:34.950+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:00:34.950+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:00:34.989+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:00:34.989+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:00:35.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.152 seconds
[2025-12-09T18:01:05.431+0000] {processor.py:161} INFO - Started process (PID=1257) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:01:05.432+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:01:05.433+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:01:05.433+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:01:05.461+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:01:05.489+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:01:05.489+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:01:05.508+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:01:05.508+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:01:05.537+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.109 seconds
[2025-12-09T18:01:35.645+0000] {processor.py:161} INFO - Started process (PID=1262) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:01:35.646+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:01:35.648+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:01:35.647+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:01:35.665+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:01:35.697+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:01:35.697+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:01:35.715+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:01:35.715+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:01:35.736+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.094 seconds
[2025-12-09T18:02:05.840+0000] {processor.py:161} INFO - Started process (PID=1324) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:02:05.842+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:02:05.843+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:02:05.843+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:02:05.871+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:02:05.925+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:02:05.925+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:02:05.958+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:02:05.958+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:02:06.001+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.166 seconds
[2025-12-09T18:02:36.754+0000] {processor.py:161} INFO - Started process (PID=1603) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:02:36.758+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:02:36.770+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:02:36.765+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:02:36.992+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:02:37.529+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:02:37.529+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:02:37.797+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:02:37.792+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:02:37.987+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.258 seconds
[2025-12-09T18:03:11.006+0000] {processor.py:161} INFO - Started process (PID=1729) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:03:11.178+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:03:11.420+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:03:11.321+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:03:15.450+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:03:46.561+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T18:04:19.686+0000] {processor.py:161} INFO - Started process (PID=1761) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:04:19.702+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:04:19.737+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:04:19.717+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:04:21.305+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:04:51.890+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T18:05:39.324+0000] {processor.py:161} INFO - Started process (PID=2002) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:05:39.338+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:05:39.351+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:05:39.347+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:05:39.697+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:05:42.005+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:05:41.993+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:05:42.197+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:05:42.195+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:05:42.329+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 3.133 seconds
[2025-12-09T18:06:13.162+0000] {processor.py:161} INFO - Started process (PID=2031) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:06:13.169+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:06:13.186+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:06:13.179+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:06:13.924+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:06:43.325+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T18:07:35.221+0000] {processor.py:161} INFO - Started process (PID=2040) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:07:35.424+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:07:35.735+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:07:35.521+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:07:47.277+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:08:16.577+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:08:16.503+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:08:52.463+0000] {processor.py:161} INFO - Started process (PID=2052) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:08:52.598+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:08:52.770+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:08:52.682+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:08:58.357+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:09:36.044+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:09:35.865+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:10:10.193+0000] {processor.py:161} INFO - Started process (PID=2083) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:10:10.235+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:10:10.309+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:10:10.264+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:10:12.517+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:15:06.127+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T18:20:47.265+0000] {processor.py:161} INFO - Started process (PID=2115) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:20:47.276+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:20:47.302+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:20:47.289+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:20:47.861+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:20:49.351+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:20:49.351+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:20:49.586+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:20:49.584+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:20:49.719+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.591 seconds
[2025-12-09T18:21:20.240+0000] {processor.py:161} INFO - Started process (PID=2207) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:21:20.265+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:21:20.310+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:21:20.291+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:21:21.585+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:21:41.467+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:21:41.417+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:21:41.729+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:21:41.705+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:21:42.946+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 22.748 seconds
[2025-12-09T18:21:51.431+0000] {processor.py:161} INFO - Started process (PID=2222) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:21:51.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:21:51.452+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:21:51.446+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:21:51.725+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:21:59.805+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:21:59.783+0000] {taskinstance.py:2700} ERROR - {'DAG Id': 'email_intelligence_pipeline', 'Task Id': 'real_time_enrichment', 'Run Id': 'manual__2025-12-09T18:00:40.606289+00:00', 'Hostname': 'e428b2e16328'}
[2025-12-09T18:22:00.056+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:22:00.046+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=email_intelligence_pipeline, task_id=real_time_enrichment, execution_date=20251209T180040, start_date=20251209T182103, end_date=20251209T182159
[2025-12-09T18:22:00.207+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: email_intelligence_pipeline.real_time_enrichment manual__2025-12-09T18:00:40.606289+00:00 [failed]> in state failed
[2025-12-09T18:22:01.019+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:22:00.928+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:22:02.055+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:22:02.013+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:22:03.732+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 12.329 seconds
[2025-12-09T18:22:38.881+0000] {processor.py:161} INFO - Started process (PID=2248) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:22:39.163+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:22:39.441+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:22:39.345+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:22:53.043+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:22:56.843+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:22:56.792+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:22:57.793+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:22:57.771+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:22:58.841+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 24.549 seconds
[2025-12-09T18:23:36.526+0000] {processor.py:161} INFO - Started process (PID=2255) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:23:36.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:23:36.560+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:23:36.550+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:23:36.936+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:24:06.749+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T18:24:37.870+0000] {processor.py:161} INFO - Started process (PID=2420) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:24:37.879+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:24:37.900+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:24:37.887+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:24:38.248+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:24:39.441+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:24:39.440+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:24:39.610+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:24:39.608+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:24:39.704+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.912 seconds
[2025-12-09T18:25:10.765+0000] {processor.py:161} INFO - Started process (PID=2455) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:25:10.767+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:25:10.769+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:25:10.768+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:25:10.809+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:25:10.871+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:25:10.871+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:25:10.896+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:25:10.895+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:25:10.919+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.160 seconds
[2025-12-09T18:25:41.275+0000] {processor.py:161} INFO - Started process (PID=2460) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:25:41.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:25:41.285+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:25:41.281+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:25:41.407+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:25:41.987+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:25:41.980+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:25:42.030+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:25:42.030+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:25:42.064+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.803 seconds
[2025-12-09T18:26:12.712+0000] {processor.py:161} INFO - Started process (PID=2465) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:26:12.715+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:26:12.725+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:26:12.722+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:26:12.914+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:26:13.174+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:26:13.174+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:26:13.284+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:26:13.284+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:26:13.333+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.659 seconds
[2025-12-09T18:26:43.584+0000] {processor.py:161} INFO - Started process (PID=2477) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:26:43.586+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:26:43.587+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:26:43.587+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:26:43.618+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:26:43.710+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:26:43.709+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:26:43.747+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:26:43.747+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:26:43.784+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.208 seconds
[2025-12-09T18:27:15.380+0000] {processor.py:161} INFO - Started process (PID=2560) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:27:15.583+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:27:15.646+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:27:15.627+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:27:17.519+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:27:46.163+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:27:46.136+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:27:46.565+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:27:46.554+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:27:47.385+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 33.708 seconds
[2025-12-09T18:27:56.106+0000] {processor.py:161} INFO - Started process (PID=2576) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:27:56.119+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:27:56.126+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:27:56.121+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:27:56.454+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:28:02.592+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:28:02.490+0000] {taskinstance.py:2700} ERROR - {'DAG Id': 'email_intelligence_pipeline', 'Task Id': 'real_time_enrichment', 'Run Id': 'manual__2025-12-09T18:26:26.557024+00:00', 'Hostname': 'e428b2e16328'}
[2025-12-09T18:28:05.051+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:28:05.024+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=email_intelligence_pipeline, task_id=real_time_enrichment, execution_date=20251209T182626, start_date=20251209T182658, end_date=20251209T182803
[2025-12-09T18:28:05.634+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: email_intelligence_pipeline.real_time_enrichment manual__2025-12-09T18:26:26.557024+00:00 [failed]> in state failed
[2025-12-09T18:28:06.669+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:28:06.637+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T18:28:07.173+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:28:07.121+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T18:28:09.641+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 13.557 seconds
[2025-12-09T18:30:47.455+0000] {processor.py:161} INFO - Started process (PID=2589) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:30:47.642+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:30:47.981+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:30:47.827+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:30:50.480+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:31:22.413+0000] {processor.py:161} INFO - Started process (PID=2689) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:31:22.540+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:31:22.899+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:31:22.752+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:31:24.792+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:32:03.198+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T18:33:26.619+0000] {processor.py:161} INFO - Started process (PID=2697) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:33:27.824+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T18:33:33.995+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:33:29.184+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T18:34:12.892+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:34:08.036+0000] {timeout.py:68} ERROR - Process timed out, PID: 2697
[2025-12-09T18:35:16.206+0000] {logging_mixin.py:188} INFO - [2025-12-09T18:34:23.810+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/email_pipeline_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/email_pipeline_dag.py", line 17, in <module>
    with DAG(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 601, in __init__
    self.timetable = create_timetable(schedule_interval, self.timezone)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 229, in create_timetable
    return CronDataIntervalTimetable(interval, timezone)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/timetables/_cron.py", line 78, in __init__
    if len(croniter(self._expression).expanded) > 5:
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 187, in __init__
    self.expanded, self.nth_weekday_of_month = self.expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 811, in expand
    return cls._expand(expr_format, hash_id=hash_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/croniter/croniter.py", line 694, in _expand
    t = re.sub(r'^\*(\/.+)$', r'%d-%d\1' % (
  File "/usr/local/lib/python3.8/re.py", line 210, in sub
    return _compile(pattern, flags).sub(repl, string, count)
  File "/usr/local/lib/python3.8/re.py", line 328, in _subx
    if not template[0] and len(template[1]) == 1:
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/email_pipeline_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 2697
[2025-12-09T18:35:32.770+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:24:37.951+0000] {processor.py:161} INFO - Started process (PID=170) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:24:37.954+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T19:24:37.956+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:24:37.956+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:24:37.995+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:24:38.036+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:24:38.036+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T19:24:38.065+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:24:38.065+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T19:24:38.101+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.154 seconds
[2025-12-09T19:25:08.321+0000] {processor.py:161} INFO - Started process (PID=175) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:25:08.324+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T19:25:08.333+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:25:08.329+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:25:08.466+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:25:09.102+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:25:09.102+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T19:25:09.163+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:25:09.163+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T19:25:09.196+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.909 seconds
[2025-12-09T19:25:39.395+0000] {processor.py:161} INFO - Started process (PID=191) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:25:39.397+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T19:25:39.399+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:25:39.398+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:25:39.482+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:25:39.738+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:25:39.738+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T19:25:39.935+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:25:39.934+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T19:25:40.053+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 0.667 seconds
[2025-12-09T19:26:10.382+0000] {processor.py:161} INFO - Started process (PID=404) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:26:10.390+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T19:26:10.403+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:26:10.397+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:26:10.606+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:26:10.919+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:26:10.918+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T19:26:11.080+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:26:11.077+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T19:26:11.383+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.028 seconds
[2025-12-09T19:26:41.907+0000] {processor.py:161} INFO - Started process (PID=565) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:26:41.916+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T19:26:41.930+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:26:41.927+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:26:42.287+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:26:42.605+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:26:42.604+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T19:26:42.765+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:26:42.763+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T19:26:42.861+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 1.077 seconds
[2025-12-09T19:27:13.545+0000] {processor.py:161} INFO - Started process (PID=841) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:27:13.571+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T19:27:13.597+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:27:13.592+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:27:14.275+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:27:15.360+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:27:15.358+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T19:27:15.754+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:27:15.748+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T19:27:15.991+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 2.763 seconds
[2025-12-09T19:27:48.384+0000] {processor.py:161} INFO - Started process (PID=923) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:27:48.441+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T19:27:48.583+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:27:48.533+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:27:52.115+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:28:03.322+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:28:03.317+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T19:28:07.047+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:28:07.037+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T19:28:10.031+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 22.540 seconds
[2025-12-09T19:28:41.591+0000] {processor.py:161} INFO - Started process (PID=987) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:28:41.643+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T19:28:41.952+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:28:41.822+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:28:46.980+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:29:17.281+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T19:29:52.247+0000] {processor.py:161} INFO - Started process (PID=1013) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:29:52.303+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T19:29:52.478+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:29:52.359+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:29:56.020+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:30:15.101+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:30:15.063+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T19:30:16.366+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:30:16.342+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T19:30:17.618+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 28.824 seconds
[2025-12-09T19:30:59.503+0000] {processor.py:161} INFO - Started process (PID=1023) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:30:59.603+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T19:31:00.301+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:31:00.142+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:31:20.004+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:31:30.985+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:31:30.612+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-12-09T19:31:37.640+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:31:37.503+0000] {dag.py:3823} INFO - Setting next_dagrun for email_intelligence_pipeline to 2025-12-09 02:00:00+00:00, run_after=2025-12-10 02:00:00+00:00
[2025-12-09T19:31:40.189+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/email_pipeline_dag.py took 47.243 seconds
[2025-12-09T19:32:15.146+0000] {processor.py:161} INFO - Started process (PID=1031) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:32:15.235+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T19:32:15.400+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:32:15.323+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:32:19.643+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:32:48.874+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-12-09T19:33:20.180+0000] {processor.py:161} INFO - Started process (PID=1036) to work on /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:33:20.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/email_pipeline_dag.py for tasks to queue
[2025-12-09T19:33:20.444+0000] {logging_mixin.py:188} INFO - [2025-12-09T19:33:20.346+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:33:23.554+0000] {processor.py:840} INFO - DAG(s) 'email_intelligence_pipeline' retrieved from /opt/airflow/dags/email_pipeline_dag.py
[2025-12-09T19:33:53.732+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 670, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
