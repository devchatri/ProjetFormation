from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
import subprocess
from airflow.models import Variable
from datetime import datetime, timedelta
import os

default_args = {
    'owner': 'data_team',                  # Qui est responsable
    'retries': 2,                          # RÃ©essayer 2 fois si erreur
    'retry_delay': timedelta(minutes=5),   # Attendre 5 min avant rÃ©essai
}


# ğŸ“§ Chemins des scripts (dans le conteneur Airflow)
GMAIL_EXTRACTOR_PATH = '/opt/airflow/kafka/Producer/email_producer.py'
EMAIL_VALIDATOR_PATH = '/opt/airflow/kafka/Consumer/validator.py'

def run_gmail_extractor(**context):
    env = build_env_vars(**context)
    subprocess.run(
        ["python", GMAIL_EXTRACTOR_PATH],
        env={**os.environ, **env},
        check=True
    )

def build_env_vars(**context):
    conf = context['dag_run'].conf if context.get('dag_run') else {}
    env_vars = {
        'KAFKA_BOOTSTRAP_SERVERS': 'kafka:29092',
        'DB_HOST': 'projetformation_postgres',
        'DB_PORT': '5432',
        'DB_NAME': 'projetformationdb',
        'DB_USER': 'postgres',
        'DB_PASSWORD': 'secret',
        'GOOGLE_CLIENT_ID': os.getenv('GOOGLE_CLIENT_ID'),
        'GOOGLE_CLIENT_SECRET': os.getenv('GOOGLE_CLIENT_SECRET')
    }
    if conf.get('email'):
        env_vars['USER_EMAIL'] = conf['email']
    if conf.get('refrechtoken'):
        env_vars['REFRECHTOKEN'] = conf['refrechtoken']
    return env_vars

with DAG(
    dag_id='email_intelligence_pipeline',  
    default_args=default_args,
    description='ğŸš€ Complete Email Intelligence Pipeline: Extract â†’ Validate â†’ Enrich â†’ Quality Check â†’ Aggregate',
    start_date=datetime(2025, 1, 1),
    schedule_interval='0 2 * * *',         # â° Chaque jour Ã  2h du matin
    catchup=False,
    tags=['email', 'intelligence', 'batch-processing', 'data-quality']
) as dag:
    # ğŸ“¬ Task 1: Gmail Extraction & Ingestion
    extract_emails = PythonOperator(
        task_id='gmail_extraction',
        python_callable=run_gmail_extractor,
        provide_context=True,
        execution_timeout=timedelta(minutes=10),
        retries=0
    )
    
    # âœ… Task 2: Email Validation & Filtering
    validate_emails = BashOperator(
        task_id='email_validation',                          
        bash_command=f'python {EMAIL_VALIDATOR_PATH}',
        env={'KAFKA_BOOTSTRAP_SERVERS': 'kafka:29092'},
        trigger_rule='all_done'
    )
    
    # ğŸ”„ Task 3a: Start real-time enrichment stream (background)
    # Lance le streaming d'enrichissement en arriÃ¨re-plan (continuera aprÃ¨s cette tÃ¢che)
    start_enrichment_stream = BashOperator(
        task_id='start_enrichment_stream',
        bash_command='''
        nohup /opt/spark/bin/spark-submit \
            --master spark://spark-master:7077 \
            --deploy-mode client \
            --packages org.apache.hadoop:hadoop-aws:3.3.4,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
            --name EmailEnrichmentStream \
            /opt/spark/jobs/email_processor.py --mode enrich > /tmp/streaming.log 2>&1 &

        # Attendre que le streaming soit prÃªt (vÃ©rifier les logs)
        sleep 15
        echo "âœ… Enrichment streaming started in background"
        ''',
        execution_timeout=timedelta(minutes=5),
        retries=0
    )

    # ğŸ”„ Task 3b: Store to Bronze (batch) â€” separate step that writes enriched data to bronze
    # This can run after the enrichment stream is started; it performs a batch write to bronze.
    store_bronze = BashOperator(
        task_id='store_bronze',
        bash_command='''
        echo "â¬‡ï¸ Storing enriched messages to bronze layer..."
        /opt/spark/bin/spark-submit \
            --master local[2] \
            --packages org.apache.hadoop:hadoop-aws:3.3.4 \
            --name StoreToBronze \
            /opt/spark/jobs/email_processor.py --mode store-bronze \
            --output s3a://datalake/bronze/emails

        EXIT_CODE=$?
        if [ $EXIT_CODE -ne 0 ]; then
            echo "âŒ store_bronze FAILED"
            exit 1
        else
            echo "âœ… store_bronze completed"
            exit 0
        fi
        ''',
        execution_timeout=timedelta(minutes=10),
        retries=1
    )
    
    # ï¿½ Task 4: Daily Insights Aggregation (Gold Layer)
    # Calcule les statistiques journaliÃ¨res par utilisateur et par date
    aggregate_insights = BashOperator(
        task_id='daily_insights_aggregation',               
        bash_command='''
        echo "ğŸ“Š Lancement de l'agrÃ©gation journaliÃ¨re..."
        
        /opt/spark/bin/spark-submit \
            --master local[2] \
            --packages org.apache.hadoop:hadoop-aws:3.3.4 \
            --name DailyInsightsAggregation \
            /opt/spark/jobs/daily_aggregation.py \
            s3a://datalake/bronze/emails \
            s3a://datalake/gold/daily_stats
        
        AGGREGATION_EXIT_CODE=$?
        
        if [ $AGGREGATION_EXIT_CODE -ne 0 ]; then
            echo "âŒ Aggregation FAILED"
            exit 1
        else
            echo "âœ… Aggregation completed successfully"
            exit 0
        fi
        ''',
        execution_timeout=timedelta(minutes=20),
        retries=1
    )
    
    # ï¿½ Define Pipeline Flow
    # Flow: extract -> validate -> start enrichment stream -> store to bronze -> aggregate
    extract_emails >> validate_emails >> start_enrichment_stream >> store_bronze >> aggregate_insights