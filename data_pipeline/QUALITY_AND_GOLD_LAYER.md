# ğŸ“Š Data Quality Check & Gold Layer Implementation

## Vue d'ensemble

Cette Ã©tape implÃ©mente la **validation de qualitÃ© des donnÃ©es** et l'**agrÃ©gation quotidienne** pour crÃ©er la **couche Gold** (Gold Layer) de notre data lake.

---

## ğŸ—ï¸ Architecture Pipeline ComplÃ¨te

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gmail API      â”‚â”€â”€â”€â”€â–¶â”‚    Kafka     â”‚â”€â”€â”€â”€â–¶â”‚  Bronze Layer   â”‚
â”‚  (Extraction)   â”‚     â”‚  (Streaming) â”‚     â”‚  (Raw Enriched) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â–¼
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚ Quality Check   â”‚
                                              â”‚  (â‰¥ 95% Score)  â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                           â”‚ PASS       â”‚   FAIL   â”‚
                                           â–¼            â–¼          
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Pipeline Fails
                                   â”‚ Gold Layer  â”‚
                                   â”‚ (Daily Stats)â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚ PostgreSQL  â”‚
                                   â”‚  (Export)   â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Fichiers CrÃ©Ã©s/ModifiÃ©s

### 1. **data_quality_check.py** (Spark Job)
**Emplacement**: `data_pipeline/spark/jobs/data_quality_check.py`

**RÃ´le**: Valider la qualitÃ© des donnÃ©es dans la couche Bronze

**FonctionnalitÃ©s**:
- âœ… **Completeness Check** (50% du score): VÃ©rifie que tous les champs requis sont prÃ©sents
  - Champs requis: `id`, `from`, `to`, `subject`, `body`, `date`, `sender_domain`
- âœ… **Freshness Check** (30% du score): VÃ©rifie que les donnÃ©es sont rÃ©centes (< 24h)
- âœ… **Accuracy Check** (20% du score): VÃ©rifie la validitÃ© des donnÃ©es
  - `email_length > 0` (pas de body vide)
  - `sender_domain` contient un `.` (format valide)
  - `subject` n'est pas vide

**Sortie**:
- Score global calculÃ© avec pondÃ©ration: `(50% Ã— completeness) + (30% Ã— freshness) + (20% Ã— accuracy)`
- Rapport JSON sauvegardÃ© dans `/tmp/quality_report.json`
- **Exit code 0** si score â‰¥ 95% (PASS)
- **Exit code 1** si score < 95% (FAIL) â†’ Pipeline s'arrÃªte

**Exemple de rapport**:
```json
{
  "timestamp": "2025-01-17T10:30:00",
  "total_records": 1000,
  "completeness": {
    "score": 98.5
  },
  "freshness": {
    "score": 100.0
  },
  "accuracy": {
    "score": 97.2
  },
  "overall_score": 98.4,
  "status": "PASS"
}
```

---

### 2. **daily_aggregation.py** (Spark Job)
**Emplacement**: `data_pipeline/spark/jobs/daily_aggregation.py`

**RÃ´le**: CrÃ©er des statistiques quotidiennes pour la couche Gold

**FonctionnalitÃ©s**:
1. **Total Emails par jour**: Compte le nombre total d'emails reÃ§us
2. **Top 10 Sender Domains**: Classement des 10 domaines qui envoient le plus d'emails
3. **Busiest Hour**: L'heure de la journÃ©e avec le plus d'activitÃ©

**Exemple de donnÃ©es Gold**:
| date       | total_emails | domain_counts                        | busiest_hour | busiest_hour_count |
|------------|--------------|--------------------------------------|--------------|--------------------|
| 2025-01-17 | 1245         | gmail.com:450\|linkedin.com:320\|... | 14           | 203                |

**Structure de sortie**:
```
s3a://datalake/gold/daily_summary/
â”œâ”€â”€ date=2025-01-17/
â”‚   â””â”€â”€ part-00000.parquet
â”œâ”€â”€ date=2025-01-18/
â”‚   â””â”€â”€ part-00000.parquet
â””â”€â”€ date=2025-01-19/
    â””â”€â”€ part-00000.parquet
```

**Avantages du partitionnement par date**:
- âœ… Pas de duplications (mode `overwrite` par date)
- âœ… Historique prÃ©servÃ© (chaque date dans un dossier sÃ©parÃ©)
- âœ… RequÃªtes rapides (partition pruning)

---

### 3. **gold_to_postgres.py** (Spark Job)
**Emplacement**: `data_pipeline/spark/jobs/gold_to_postgres.py`

**RÃ´le**: Exporter les statistiques Gold vers PostgreSQL

**FonctionnalitÃ©s**:
- Lit les donnÃ©es agrÃ©gÃ©es depuis `s3a://datalake/gold/daily_summary/`
- Exporte vers la table PostgreSQL `email_daily_summary`
- Mode `overwrite` pour garder uniquement les donnÃ©es les plus rÃ©centes

**Configuration PostgreSQL**:
```python
db_url = "jdbc:postgresql://postgres:5432/airflow"
db_properties = {
    "user": "airflow",
    "password": "airflow",
    "driver": "org.postgresql.Driver"
}
```

---

### 4. **email_pipeline_dag.py** (Airflow DAG - MODIFIÃ‰)
**Emplacement**: `data_pipeline/airflow/dags/email_pipeline_dag.py`

**Modifications**:
- âœ… DÃ©commentÃ© les tÃ¢ches `quality_gate`, `aggregate_insights`, `export_postgres`
- âœ… Mis Ã  jour le pipeline flow complet
- âœ… SimplifiÃ© la logique de quality gate (exit 1 si fail au lieu de skip silencieux)

**Nouveau Pipeline Flow**:
```
extract_emails 
    â†“
validate_emails 
    â†“
enrich_stream (Bronze + Silver)
    â†“
quality_gate (â‰¥ 95%)
    â†“
aggregate_insights (Gold)
    â†“
export_postgres
```

**DÃ©tails des tÃ¢ches**:

#### Task 4: `quality_assurance_gate`
```bash
/opt/spark/bin/spark-submit --master local[2] \
    --packages org.apache.hadoop:hadoop-aws:3.3.4 \
    --name QualityAssuranceGate \
    /opt/spark/jobs/data_quality_check.py
```
- **Timeout**: 20 minutes
- **Retries**: 0 (fail fast si qualitÃ© insuffisante)
- **Trigger Rule**: `all_success` (par dÃ©faut)

#### Task 5: `daily_insights_aggregation`
```bash
/opt/spark/bin/spark-submit --master local[2] \
    --packages org.apache.hadoop:hadoop-aws:3.3.4 \
    --name DailyInsightsAggregation \
    /opt/spark/jobs/daily_aggregation.py
```
- **Timeout**: 20 minutes
- **DÃ©pendance**: S'exÃ©cute seulement si `quality_gate` rÃ©ussit

#### Task 6: `export_to_postgres`
```bash
/opt/spark/bin/spark-submit --master local[2] \
    --packages org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.7.1 \
    --name GoldToPostgres \
    /opt/spark/jobs/gold_to_postgres.py
```
- **Timeout**: 15 minutes
- **DÃ©pendance**: S'exÃ©cute seulement si `aggregate_insights` rÃ©ussit

---

## ğŸš€ Comment Tester

### 1. VÃ©rifier que le DAG est chargÃ©
```bash
# Se connecter au conteneur Airflow
docker exec -it airflow-scheduler bash

# VÃ©rifier que le DAG est sans erreurs
airflow dags list | grep email_intelligence_pipeline

# VÃ©rifier les tÃ¢ches du DAG
airflow tasks list email_intelligence_pipeline
```

**Sortie attendue**:
```
gmail_extraction
email_validation
enrich_bronze_silver
quality_assurance_gate
daily_insights_aggregation
export_to_postgres
```

### 2. DÃ©clencher le pipeline manuellement
```bash
# DÃ©clencher le DAG avec une configuration spÃ©cifique
curl -X POST "http://localhost:8080/api/v1/dags/email_intelligence_pipeline/dagRuns" \
  -H "Content-Type: application/json" \
  -u admin:admin \
  -d '{
    "conf": {
      "email": "votre.email@gmail.com",
      "refrechtoken": "votre_refresh_token"
    }
  }'
```

### 3. Tester individuellement chaque Spark job

#### Quality Check
```bash
docker exec -it spark-master bash
/opt/spark/bin/spark-submit \
    --master local[2] \
    --packages org.apache.hadoop:hadoop-aws:3.3.4 \
    /opt/spark/jobs/data_quality_check.py
```

**RÃ©sultat attendu**:
```
ğŸš€ Starting Data Quality Check
ğŸ“‹ Required fields: id, from, to, subject, body, date, sender_domain
ğŸ“ˆ Quality threshold: 95%

ğŸ” CHECK 1: COMPLETENESS
  Score: 98.50%

ğŸ” CHECK 2: FRESHNESS
  Score: 100.00%

ğŸ” CHECK 3: ACCURACY
  Score: 97.20%

ğŸ“ˆ OVERALL QUALITY SCORE: 98.40%
âœ… PASS: Quality 98.40% >= 95%
```

#### Daily Aggregation
```bash
/opt/spark/bin/spark-submit \
    --master local[2] \
    --packages org.apache.hadoop:hadoop-aws:3.3.4 \
    /opt/spark/jobs/daily_aggregation.py
```

**RÃ©sultat attendu**:
```
ğŸš€ DAILY AGGREGATION JOB STARTED

ğŸ“Œ Step 1: Reading Bronze layer from MinIO...
âœ… Bronze layer loaded successfully: 1245 records

ğŸ“Œ Step 2: Creating daily aggregations...
   â€¢ Calculating total emails per day...
   â€¢ Calculating top 10 sender domains...
   â€¢ Calculating busiest hour...
âœ… Daily aggregations created successfully: 1 records

ğŸ“Œ Step 3: Writing aggregations to Gold layer...
âœ… Gold layer written successfully!
   â””â”€ Location: s3a://datalake/gold/daily_summary/date=2025-01-17/

âœ… DAILY AGGREGATION JOB COMPLETED SUCCESSFULLY
```

#### Export PostgreSQL
```bash
/opt/spark/bin/spark-submit \
    --master local[2] \
    --packages org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.7.1 \
    /opt/spark/jobs/gold_to_postgres.py
```

### 4. VÃ©rifier les donnÃ©es dans MinIO

```bash
# Se connecter au conteneur MinIO
docker exec -it minio mc ls minio/datalake/gold/daily_summary/

# Lister les partitions par date
mc ls minio/datalake/gold/daily_summary/ --recursive
```

**Structure attendue**:
```
[2025-01-17 10:30:00] date=2025-01-17/
[2025-01-18 10:30:00] date=2025-01-18/
[2025-01-19 10:30:00] date=2025-01-19/
```

### 5. VÃ©rifier les donnÃ©es dans PostgreSQL

```bash
# Se connecter Ã  PostgreSQL
docker exec -it postgres psql -U airflow -d airflow

# Voir le schÃ©ma de la table
\d email_daily_summary

# Voir les donnÃ©es
SELECT * FROM email_daily_summary ORDER BY date DESC LIMIT 5;
```

**RÃ©sultat attendu**:
```
    date    | total_emails |          domain_counts          | busiest_hour | busiest_hour_count
------------+--------------+---------------------------------+--------------+--------------------
 2025-01-19 |         1532 | gmail.com:580|linkedin.com:420  |           14 |                245
 2025-01-18 |         1245 | gmail.com:450|outlook.com:320   |           10 |                203
 2025-01-17 |         1103 | gmail.com:400|yahoo.com:290     |           15 |                178
```

---

## ğŸ“Š MÃ©triques de QualitÃ©

### Seuils de qualitÃ© configurÃ©s

| MÃ©trique      | PondÃ©ration | Seuil Minimum | Description                        |
|---------------|-------------|---------------|------------------------------------|
| Completeness  | 50%         | N/A           | Tous les champs requis prÃ©sents    |
| Freshness     | 30%         | < 24h         | DonnÃ©es rÃ©centes                   |
| Accuracy      | 20%         | N/A           | Formats valides                    |
| **Overall**   | **100%**    | **â‰¥ 95%**     | **Score global pour passer gate**  |

### Que se passe-t-il si qualitÃ© < 95% ?

1. âŒ Task `quality_assurance_gate` **FAIL** (exit code 1)
2. â›” Tasks `daily_insights_aggregation` et `export_to_postgres` sont **SKIPPED**
3. ğŸ”” Airflow envoie une notification d'Ã©chec
4. ğŸ“Š Rapport JSON disponible dans `/tmp/quality_report.json` pour diagnostic

**Action corrective**:
- Identifier les problÃ¨mes de qualitÃ© dans le rapport
- Corriger les donnÃ©es source (emails manquants, formats invalides, etc.)
- Relancer le DAG

---

## ğŸ” Monitoring & Debugging

### Logs Airflow
```bash
# Voir les logs d'une tÃ¢che spÃ©cifique
docker exec -it airflow-scheduler bash
cat /opt/airflow/logs/dag_id=email_intelligence_pipeline/run_id=*/task_id=quality_assurance_gate/attempt=1.log
```

### Logs Spark
```bash
# Quality Check logs
docker exec -it spark-master cat /tmp/quality_report.json

# Daily Aggregation logs
docker exec -it spark-master cat /opt/spark/work-dir/daily_aggregation.log
```

### MinIO Console
AccÃ©der via: http://localhost:9001
- **Username**: minioadmin
- **Password**: minioadmin123

---

## ğŸ“ˆ AmÃ©liorations Futures

1. **Alertes Slack/Email** si qualitÃ© < 95%
2. **Visualisation Grafana** des mÃ©triques de qualitÃ©
3. **Anomaly Detection** sur les agrÃ©gations (dÃ©tection de pics anormaux)
4. **Retention Policy** sur les donnÃ©es Bronze/Silver (supprimer aprÃ¨s 30 jours)
5. **Schema Evolution** avec Delta Lake pour versioning

---

## âœ… Checklist de DÃ©ploiement

- [x] Fichiers Spark jobs crÃ©Ã©s (`data_quality_check.py`, `daily_aggregation.py`, `gold_to_postgres.py`)
- [x] DAG Airflow mis Ã  jour avec les nouvelles tÃ¢ches
- [x] Pipeline flow configurÃ©: Extract â†’ Validate â†’ Enrich â†’ Quality â†’ Aggregate â†’ Export
- [x] Quality gate avec seuil 95%
- [x] Partitionnement Gold layer par date
- [ ] Tests unitaires pour les Spark jobs
- [ ] Documentation utilisateur
- [ ] Monitoring & alerting configurÃ©

---

## ğŸ¯ RÃ©sumÃ©

Cette implÃ©mentation ajoute:
1. âœ… **Validation de qualitÃ©** automatique avec gate Ã  95%
2. âœ… **AgrÃ©gations quotidiennes** (Gold layer) avec statistiques business
3. âœ… **Export PostgreSQL** pour visualisation/reporting
4. âœ… **Pipeline complet** de bout en bout avec orchestration Airflow

Le systÃ¨me garantit maintenant que seules les donnÃ©es de **haute qualitÃ©** sont propagÃ©es vers les couches supÃ©rieures et l'export analytique.
