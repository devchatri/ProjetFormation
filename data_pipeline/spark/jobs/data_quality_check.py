#!/usr/bin/env python3
"""
Data Quality Check Script
V√©rifie la qualit√© des emails dans la couche Bronze
- Premier lancement : v√©rifie tous les emails
- Lancements suivants : v√©rifie uniquement les nouveaux emails (non v√©rifi√©s)

Aspects v√©rifi√©s :
1. Compl√©tude : toutes les colonnes obligatoires sont remplies
2. Validit√© : format email correct, body non vide
3. Fra√Æcheur : emails r√©cents (< 7 jours)
"""

import sys
import json
import os
from datetime import datetime, timedelta
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, datediff, current_date

# Seuil de qualit√© minimum (95%)
QUALITY_THRESHOLD = 95.0

# Fichier checkpoint pour suivre les emails d√©j√† v√©rifi√©s
CHECKPOINT_FILE = "/opt/airflow/quality_checkpoint.json"


def create_spark_session():
    """Cr√©e une session Spark avec support S3/MinIO"""
    return SparkSession.builder \
        .appName("DataQualityCheck") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.sql.files.ignoreCorruptFiles", "true") \
        .config("spark.sql.files.ignoreMissingFiles", "true") \
        .getOrCreate()


def load_checkpoint():
    """Charge le checkpoint des emails d√©j√† v√©rifi√©s"""
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, 'r') as f:
            return json.load(f)
    return {
        "last_check": None,
        "verified_ids": [],
        "total_verified": 0
    }


def save_checkpoint(checkpoint):
    """Sauvegarde le checkpoint"""
    checkpoint["last_check"] = datetime.utcnow().isoformat()
    with open(CHECKPOINT_FILE, 'w') as f:
        json.dump(checkpoint, f, indent=2)


def check_completeness(df):
    """V√©rifie que les colonnes obligatoires sont remplies"""
    total = df.count()
    if total == 0:
        return 100.0, {"total": 0, "complete": 0, "incomplete": 0}
    
    # Colonnes obligatoires
    complete = df.filter(
        (col("id").isNotNull()) &
        (col("from").isNotNull()) &
        (col("to").isNotNull()) &
        (col("subject").isNotNull()) &
        (col("body").isNotNull()) &
        (col("user_email").isNotNull())
    ).count()
    
    score = (complete / total) * 100
    
    return score, {
        "total": total,
        "complete": complete,
        "incomplete": total - complete,
        "percentage": round(score, 2)
    }


def check_validity(df):
    """V√©rifie le format des emails et que le body n'est pas vide"""
    total = df.count()
    if total == 0:
        return 100.0, {"total": 0, "valid": 0, "invalid": 0}
    
    valid = df.filter(
        (col("from").contains("@")) &
        (col("to").contains("@")) &
        (col("body") != "") &
        (col("body").isNotNull())
    ).count()
    
    score = (valid / total) * 100
    
    return score, {
        "total": total,
        "valid": valid,
        "invalid": total - valid,
        "percentage": round(score, 2)
    }


def check_freshness(df):
    """V√©rifie que les emails sont r√©cents (< 7 jours)"""
    total = df.count()
    if total == 0:
        return 100.0, {"total": 0, "recent": 0, "old": 0}
    
    # Emails des 7 derniers jours
    recent = df.filter(
        datediff(current_date(), col("date")) <= 7
    ).count()
    
    score = (recent / total) * 100
    
    return score, {
        "total": total,
        "recent": recent,
        "old": total - recent,
        "percentage": round(score, 2)
    }


def generate_quality_report(spark, report_path):
    """G√©n√®re le rapport de qualit√©"""
    print("üîç Starting Data Quality Check...")
    print(f"‚è∞ Time: {datetime.utcnow().isoformat()}\n")
    
    # Charger le checkpoint
    checkpoint = load_checkpoint()
    is_first_run = checkpoint["last_check"] is None
    
    if is_first_run:
        print("üìå PREMIER LANCEMENT : V√©rification de tous les emails")
    else:
        print(f"üìå LANCEMENT SUIVANT : V√©rification des nouveaux emails")
        print(f"   Derni√®re v√©rification : {checkpoint['last_check']}")
        print(f"   Emails d√©j√† v√©rifi√©s : {checkpoint['total_verified']}\n")
    
    # Lire les donn√©es Bronze
    bronze_path = "s3a://datalake/bronze/emails/"
    
    try:
        df = spark.read.format("parquet").load(bronze_path)
        total_rows = df.count()
        print(f"‚úÖ {total_rows} emails trouv√©s dans Bronze\n")
        
        if total_rows == 0:
            print("‚ö†Ô∏è Aucune donn√©e √† v√©rifier")
            report = {
                "timestamp": datetime.utcnow().isoformat(),
                "status": "NO_DATA",
                "overall_score": 0.0,
                "message": "Aucune donn√©e dans Bronze"
            }
            with open(report_path, "w") as f:
                json.dump(report, f, indent=2)
            return False
        
        # Filtrer les nouveaux emails (non v√©rifi√©s)
        if not is_first_run and checkpoint["verified_ids"]:
            df_to_check = df.filter(~col("id").isin(checkpoint["verified_ids"]))
            new_count = df_to_check.count()
            print(f"üîé {new_count} nouveaux emails √† v√©rifier")
        else:
            df_to_check = df
            new_count = total_rows
            print(f"üîé V√©rification de tous les {total_rows} emails")
        
        if new_count == 0:
            print("‚úÖ Aucun nouvel email √† v√©rifier")
            report = {
                "timestamp": datetime.utcnow().isoformat(),
                "status": "NO_NEW_DATA",
                "overall_score": 100.0,
                "message": "Aucun nouvel email depuis la derni√®re v√©rification"
            }
            with open(report_path, "w") as f:
                json.dump(report, f, indent=2)
            return True
        
        # Ex√©cuter les checks
        print("\nüìä V√©rification de la qualit√©...")
        print("="*60)
        
        completeness_score, completeness_details = check_completeness(df_to_check)
        print(f"‚úì Compl√©tude : {completeness_score:.2f}%")
        
        validity_score, validity_details = check_validity(df_to_check)
        print(f"‚úì Validit√©   : {validity_score:.2f}%")
        
        freshness_score, freshness_details = check_freshness(df_to_check)
        print(f"‚úì Fra√Æcheur  : {freshness_score:.2f}%")
        
        # Score global (moyenne pond√©r√©e)
        overall_score = (
            completeness_score * 0.40 +  # 40% compl√©tude
            validity_score * 0.40 +       # 40% validit√©
            freshness_score * 0.20        # 20% fra√Æcheur
        )
        
        status = "PASSED" if overall_score >= QUALITY_THRESHOLD else "FAILED"
        
        print("="*60)
        print(f"\nüìã R√âSULTAT : {status}")
        print(f"   Score global : {overall_score:.2f}% (seuil: {QUALITY_THRESHOLD}%)")
        
        # G√©n√©rer le rapport
        report = {
            "timestamp": datetime.utcnow().isoformat(),
            "status": status,
            "overall_score": round(overall_score, 2),
            "threshold": QUALITY_THRESHOLD,
            "emails_checked": new_count,
            "is_first_run": is_first_run,
            "checks": {
                "completeness": {
                    "score": round(completeness_score, 2),
                    "weight": 0.40,
                    "details": completeness_details
                },
                "validity": {
                    "score": round(validity_score, 2),
                    "weight": 0.40,
                    "details": validity_details
                },
                "freshness": {
                    "score": round(freshness_score, 2),
                    "weight": 0.20,
                    "details": freshness_details
                }
            }
        }
        
        # Sauvegarder le rapport
        with open(report_path, "w") as f:
            json.dump(report, f, indent=2)
        
        print(f"\nüíæ Rapport sauvegard√© : {report_path}\n")
        
        # Mettre √† jour le checkpoint
        if status == "PASSED":
            new_ids = [row.id for row in df_to_check.select("id").collect()]
            checkpoint["verified_ids"].extend(new_ids)
            checkpoint["total_verified"] = len(checkpoint["verified_ids"])
            save_checkpoint(checkpoint)
            print(f"‚úÖ Checkpoint mis √† jour : {checkpoint['total_verified']} emails v√©rifi√©s au total\n")
        
        return status == "PASSED"
        
    except Exception as e:
        print(f"\n‚ùå Erreur : {e}")
        import traceback
        traceback.print_exc()
        
        report = {
            "timestamp": datetime.utcnow().isoformat(),
            "status": "ERROR",
            "overall_score": 0.0,
            "error": str(e)
        }
        with open(report_path, "w") as f:
            json.dump(report, f, indent=2)
        
        return False


def main():
    if len(sys.argv) < 2:
        print("Usage: spark-submit data_quality_check.py <report_path>")
        print("Exemple: spark-submit data_quality_check.py quality_report.json")
        sys.exit(1)
    
    report_path = sys.argv[1]
    
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    try:
        passed = generate_quality_report(spark, report_path)
        
        if passed:
            print("="*60)
            print("‚úÖ QUALITY CHECK PASSED")
            print("="*60)
            sys.exit(0)
        else:
            print("="*60)
            print("‚ùå QUALITY CHECK FAILED")
            print("="*60)
            sys.exit(1)
    
    except Exception as e:
        print(f"‚ùå Erreur fatale : {e}")
        sys.exit(1)
    
    finally:
        spark.stop()


if __name__ == "__main__":
    main()
